{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a3e6950",
   "metadata": {},
   "source": [
    "# Mixture of Experts (MoE) Implementation from Scratch\n",
    "\n",
    "## Overview\n",
    "This notebook implements a complete Mixture of Experts architecture from scratch using only PyTorch. We'll build:\n",
    "\n",
    "1. **Expert Networks**: Individual feed-forward neural networks\n",
    "2. **Router Mechanism**: Top-K routing with load balancing\n",
    "3. **Sparse MoE Layer**: Combining expert outputs based on routing weights\n",
    "4. **Complete Transformer**: Full transformer with MoE replacing the FFN layer\n",
    "5. **Training Pipeline**: Pre-training on Shakespeare dataset\n",
    "6. **Inference**: Text generation with the trained MoE model\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand MoE architecture components\n",
    "- Implement routing mechanisms and load balancing\n",
    "- Build sparse computation patterns\n",
    "- Train and evaluate MoE models\n",
    "- Compare with traditional dense transformers\n",
    "\n",
    "## Architecture Overview\n",
    "```\n",
    "Input â†’ Multi-Head Attention â†’ Layer Norm â†’ MoE Layer â†’ Output\n",
    "                â†‘                              â†“\n",
    "           Residual Connection         Router + Experts\n",
    "```\n",
    "\n",
    "Let's start implementing each component step by step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18c73d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import Required Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(1337)\n",
    "random.seed(1337)\n",
    "np.random.seed(1337)\n",
    "\n",
    "# Check device availability\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# GPU memory optimization\n",
    "if device == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a59c3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 1,115,394 characters\n",
      "Sample text:\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n",
      "Vocabulary size: 65 characters\n",
      "Characters: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Original: Hello World!\n",
      "Encoded: [20, 43, 50, 50, 53, 1, 35, 53, 56, 50, 42, 2]\n",
      "Decoded: Hello World!\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Load and Prepare Dataset\n",
    "# Download Shakespeare dataset if not already present\n",
    "if not os.path.exists('input.txt'):\n",
    "    url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
    "    with open('input.txt', 'w', encoding='utf-8') as f:\n",
    "        f.write(requests.get(url).text)\n",
    "\n",
    "# Read the dataset\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(f\"Dataset size: {len(text):,} characters\")\n",
    "print(\"Sample text:\")\n",
    "print(text[:500])\n",
    "\n",
    "# Character-level tokenization\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f\"Vocabulary size: {vocab_size} characters\")\n",
    "print(f\"Characters: {''.join(chars)}\")\n",
    "\n",
    "# Create character-to-integer and integer-to-character mappings\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "def encode(s):\n",
    "    \"\"\"Encode string to list of integers\"\"\"\n",
    "    return [stoi[c] for c in s]\n",
    "\n",
    "def decode(l):\n",
    "    \"\"\"Decode list of integers to string\"\"\"\n",
    "    return ''.join([itos[i] for i in l])\n",
    "\n",
    "# Test encoding/decoding\n",
    "test_text = \"Hello World!\"\n",
    "encoded = encode(test_text)\n",
    "decoded = decode(encoded)\n",
    "print(f\"Original: {test_text}\")\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: {decoded}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f538f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Expert Network ===\n",
      "Input shape: torch.Size([1, 4, 8])\n",
      "Expert output shape: torch.Size([1, 4, 8])\n",
      "Expert parameters: 512\n",
      "âœ“ Expert network working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Define Individual Expert Networks\n",
    "class Expert(nn.Module):\n",
    "    \"\"\"\n",
    "    Individual expert network - a simple feed-forward neural network\n",
    "    Each expert specializes in processing certain types of tokens\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embed, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # Expansion layer: embed_dim -> 4*embed_dim (standard in transformers)\n",
    "        self.w1 = nn.Linear(n_embed, 4 * n_embed, bias=False)\n",
    "        # Contraction layer: 4*embed_dim -> embed_dim  \n",
    "        self.w2 = nn.Linear(4 * n_embed, n_embed, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, n_embed)\n",
    "        # Apply expansion, ReLU activation, contraction, and dropout\n",
    "        return self.dropout(self.w2(F.relu(self.w1(x))))\n",
    "\n",
    "# Test the Expert network\n",
    "print(\"=== Testing Expert Network ===\")\n",
    "n_embed = 8  # Embedding dimension\n",
    "expert = Expert(n_embed)\n",
    "\n",
    "# Create test input: 1 batch, 4 tokens, 8-dim embeddings\n",
    "test_input = torch.randn(1, 4, n_embed)\n",
    "expert_output = expert(test_input)\n",
    "\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Expert output shape: {expert_output.shape}\")\n",
    "print(f\"Expert parameters: {sum(p.numel() for p in expert.parameters()):,}\")\n",
    "\n",
    "# Verify the expert maintains input/output dimensions\n",
    "assert test_input.shape == expert_output.shape, \"Expert should maintain input shape\"\n",
    "print(\"âœ“ Expert network working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "291ef2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Router Mechanism ===\n",
      "Input shape: torch.Size([1, 4, 8])\n",
      "Routing weights shape: torch.Size([1, 4, 3])\n",
      "Expert indices shape: torch.Size([1, 4, 2])\n",
      "\n",
      "Routing weights (each row sums to 1.0):\n",
      "tensor([[0.4686, 0.5314, 0.0000],\n",
      "        [0.2901, 0.7099, 0.0000],\n",
      "        [0.4520, 0.5480, 0.0000],\n",
      "        [0.6347, 0.0000, 0.3653]], grad_fn=<SqueezeBackward1>)\n",
      "\n",
      "Selected expert indices:\n",
      "tensor([[1, 0],\n",
      "        [1, 0],\n",
      "        [1, 0],\n",
      "        [0, 2]])\n",
      "\n",
      "Row sums (should be 1.0): tensor([1., 1., 1., 1.], grad_fn=<SqueezeBackward1>)\n",
      "âœ“ Router working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Implement Router Mechanism\n",
    "class TopKRouter(nn.Module):\n",
    "    \"\"\"\n",
    "    Router that determines which experts should process each token\n",
    "    Uses top-k routing to select the best experts for each token\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embed, num_experts, top_k):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        # Router network: maps input to expert scores\n",
    "        self.router = nn.Linear(n_embed, num_experts, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, n_embed)\n",
    "        batch_size, seq_len, n_embed = x.shape\n",
    "        \n",
    "        # Step 1: Get routing scores (logits) for each expert\n",
    "        # Shape: (batch_size, seq_len, num_experts)\n",
    "        logits = self.router(x)\n",
    "        \n",
    "        # Step 2: Select top-k experts for each token\n",
    "        # top_k_logits: (batch_size, seq_len, top_k)  \n",
    "        # top_k_indices: (batch_size, seq_len, top_k)\n",
    "        top_k_logits, top_k_indices = torch.topk(logits, self.top_k, dim=-1)\n",
    "        \n",
    "        # Step 3: Create mask for selected experts\n",
    "        # Initialize with negative infinity (will become 0 after softmax)\n",
    "        routing_weights = torch.full_like(logits, float('-inf'))\n",
    "        \n",
    "        # Set selected expert logits\n",
    "        routing_weights.scatter_(-1, top_k_indices, top_k_logits)\n",
    "        \n",
    "        # Step 4: Apply softmax to get routing probabilities\n",
    "        # Only selected experts will have non-zero probabilities\n",
    "        routing_weights = F.softmax(routing_weights, dim=-1)\n",
    "        \n",
    "        return routing_weights, top_k_indices\n",
    "\n",
    "# Test the Router\n",
    "print(\"=== Testing Router Mechanism ===\")\n",
    "num_experts = 3\n",
    "top_k = 2\n",
    "n_embed = 8\n",
    "\n",
    "router = TopKRouter(n_embed, num_experts, top_k)\n",
    "\n",
    "# Test input: 1 batch, 4 tokens, 8-dim embeddings\n",
    "test_input = torch.randn(1, 4, n_embed)\n",
    "routing_weights, expert_indices = router(test_input)\n",
    "\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Routing weights shape: {routing_weights.shape}\")\n",
    "print(f\"Expert indices shape: {expert_indices.shape}\")\n",
    "\n",
    "print(f\"\\nRouting weights (each row sums to 1.0):\")\n",
    "print(routing_weights.squeeze(0))\n",
    "\n",
    "print(f\"\\nSelected expert indices:\")\n",
    "print(expert_indices.squeeze(0))\n",
    "\n",
    "# Verify that each row sums to 1.0 (probability distribution)\n",
    "row_sums = routing_weights.sum(dim=-1)\n",
    "print(f\"\\nRow sums (should be 1.0): {row_sums.squeeze(0)}\")\n",
    "\n",
    "print(\"âœ“ Router working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd76efdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Noisy Top-K Router ===\n",
      "Training mode routing weights (with noise):\n",
      "tensor([[0.0000, 0.5040, 0.4960],\n",
      "        [0.0000, 0.5704, 0.4296],\n",
      "        [0.4679, 0.0000, 0.5321],\n",
      "        [0.7194, 0.2806, 0.0000]], grad_fn=<SqueezeBackward1>)\n",
      "\n",
      "Evaluation mode routing weights (without noise):\n",
      "tensor([[0.0000, 0.5254, 0.4746],\n",
      "        [0.0000, 0.6359, 0.3641],\n",
      "        [0.4976, 0.0000, 0.5024],\n",
      "        [0.7319, 0.2681, 0.0000]], grad_fn=<SqueezeBackward1>)\n",
      "\n",
      "Difference (shows effect of noise):\n",
      "0.06540459394454956\n",
      "âœ“ Noisy router working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Implement Noisy Top-K Router (Advanced Load Balancing)\n",
    "class NoisyTopKRouter(nn.Module):\n",
    "    \"\"\"\n",
    "    Enhanced router with Gaussian noise for better load balancing\n",
    "    Noise helps prevent certain experts from being consistently favored\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embed, num_experts, top_k, noise_std=0.1):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.noise_std = noise_std\n",
    "        \n",
    "        # Router network\n",
    "        self.router = nn.Linear(n_embed, num_experts, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, n_embed)\n",
    "        batch_size, seq_len, n_embed = x.shape\n",
    "        \n",
    "        # Step 1: Get base routing scores\n",
    "        logits = self.router(x)\n",
    "        \n",
    "        # Step 2: Add Gaussian noise during training for load balancing\n",
    "        if self.training:\n",
    "            noise = torch.randn_like(logits) * self.noise_std\n",
    "            logits = logits + noise\n",
    "        \n",
    "        # Step 3: Select top-k experts\n",
    "        top_k_logits, top_k_indices = torch.topk(logits, self.top_k, dim=-1)\n",
    "        \n",
    "        # Step 4: Create routing weights with softmax\n",
    "        routing_weights = torch.full_like(logits, float('-inf'))\n",
    "        routing_weights.scatter_(-1, top_k_indices, top_k_logits)\n",
    "        routing_weights = F.softmax(routing_weights, dim=-1)\n",
    "        \n",
    "        return routing_weights, top_k_indices\n",
    "\n",
    "# Test Noisy Router\n",
    "print(\"=== Testing Noisy Top-K Router ===\")\n",
    "noisy_router = NoisyTopKRouter(n_embed, num_experts, top_k, noise_std=0.1)\n",
    "\n",
    "# Test in training mode (with noise)\n",
    "noisy_router.train()\n",
    "routing_weights_train, _ = noisy_router(test_input)\n",
    "\n",
    "# Test in evaluation mode (without noise)  \n",
    "noisy_router.eval()\n",
    "routing_weights_eval, _ = noisy_router(test_input)\n",
    "\n",
    "print(\"Training mode routing weights (with noise):\")\n",
    "print(routing_weights_train.squeeze(0))\n",
    "\n",
    "print(\"\\nEvaluation mode routing weights (without noise):\")\n",
    "print(routing_weights_eval.squeeze(0))\n",
    "\n",
    "print(\"\\nDifference (shows effect of noise):\")\n",
    "print((routing_weights_train - routing_weights_eval).abs().max().item())\n",
    "\n",
    "print(\"âœ“ Noisy router working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d62a74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Complete Sparse MoE Layer ===\n",
      "Input shape: torch.Size([1, 4, 8])\n",
      "MoE output shape: torch.Size([1, 4, 8])\n",
      "MoE parameters: 1,560\n",
      "\n",
      "Input sample:\n",
      "tensor([[ 2.2792, -0.3402, -0.7501,  0.2942,  0.7626,  2.6536,  0.4730,  0.0147],\n",
      "        [-0.1513,  1.5333,  1.0515, -0.4613,  2.0802,  0.8309, -0.8416, -0.1644],\n",
      "        [-1.0877,  0.0698, -1.1470, -0.5624, -0.1978,  0.8101, -1.0031,  0.6105],\n",
      "        [ 1.2420,  0.5707, -0.0135, -1.0993,  1.3919,  0.9944,  0.5453,  0.1693]])\n",
      "\n",
      "MoE output sample:\n",
      "tensor([[-0.2407,  0.0637, -0.0806,  0.1530,  0.1621, -0.2777,  0.0332,  0.2249],\n",
      "        [ 0.2400, -0.3806,  0.0677, -0.2340, -0.1102, -0.2391, -0.0081,  0.1539],\n",
      "        [-0.1661, -0.0328,  0.1538, -0.0882,  0.0367,  0.1104,  0.0214, -0.0748],\n",
      "        [-0.1191, -0.1103, -0.2469,  0.0279, -0.0493, -0.3956, -0.0544,  0.3399]],\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "âœ“ Sparse MoE layer working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Implement Complete Sparse MoE Layer\n",
    "class SparseMoE(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete Sparse Mixture of Experts layer\n",
    "    Combines routing mechanism with expert networks\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embed, num_experts, top_k, dropout=0.1, use_noise=True):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.n_embed = n_embed\n",
    "        \n",
    "        # Create expert networks\n",
    "        self.experts = nn.ModuleList([\n",
    "            Expert(n_embed, dropout) for _ in range(num_experts)\n",
    "        ])\n",
    "        \n",
    "        # Router selection\n",
    "        if use_noise:\n",
    "            self.router = NoisyTopKRouter(n_embed, num_experts, top_k)\n",
    "        else:\n",
    "            self.router = TopKRouter(n_embed, num_experts, top_k)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, n_embed)\n",
    "        batch_size, seq_len, n_embed = x.shape\n",
    "        \n",
    "        # Step 1: Get routing weights and expert indices\n",
    "        routing_weights, expert_indices = self.router(x)\n",
    "        \n",
    "        # Step 2: Process input through all experts\n",
    "        # We process all tokens through all experts, then select outputs\n",
    "        expert_outputs = []\n",
    "        for expert in self.experts:\n",
    "            expert_output = expert(x)  # Shape: (batch_size, seq_len, n_embed)\n",
    "            expert_outputs.append(expert_output)\n",
    "        \n",
    "        # Stack expert outputs: (num_experts, batch_size, seq_len, n_embed)\n",
    "        expert_outputs = torch.stack(expert_outputs, dim=0)\n",
    "        \n",
    "        # Step 3: Combine expert outputs using routing weights\n",
    "        # Reshape for efficient computation\n",
    "        x_flat = x.view(-1, n_embed)  # (batch_size * seq_len, n_embed)\n",
    "        routing_weights_flat = routing_weights.view(-1, self.num_experts)  # (batch_size * seq_len, num_experts)\n",
    "        \n",
    "        # Initialize output\n",
    "        final_output = torch.zeros_like(x_flat)\n",
    "        \n",
    "        # For each expert, add its weighted contribution\n",
    "        for expert_idx in range(self.num_experts):\n",
    "            # Get expert output for all tokens\n",
    "            expert_output_flat = expert_outputs[expert_idx].view(-1, n_embed)\n",
    "            \n",
    "            # Get routing weights for this expert\n",
    "            expert_weights = routing_weights_flat[:, expert_idx:expert_idx+1]  # (batch_size * seq_len, 1)\n",
    "            \n",
    "            # Add weighted expert output\n",
    "            final_output += expert_weights * expert_output_flat\n",
    "        \n",
    "        # Reshape back to original dimensions\n",
    "        final_output = final_output.view(batch_size, seq_len, n_embed)\n",
    "        \n",
    "        return final_output\n",
    "\n",
    "# Test the complete MoE layer\n",
    "print(\"=== Testing Complete Sparse MoE Layer ===\")\n",
    "moe_layer = SparseMoE(\n",
    "    n_embed=8,\n",
    "    num_experts=3, \n",
    "    top_k=2,\n",
    "    dropout=0.1,\n",
    "    use_noise=True\n",
    ")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randn(1, 4, 8)\n",
    "moe_output = moe_layer(test_input)\n",
    "\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"MoE output shape: {moe_output.shape}\")\n",
    "print(f\"MoE parameters: {sum(p.numel() for p in moe_layer.parameters()):,}\")\n",
    "\n",
    "# Verify output shape matches input shape\n",
    "assert test_input.shape == moe_output.shape, \"MoE should maintain input shape\"\n",
    "\n",
    "print(f\"\\nInput sample:\")\n",
    "print(test_input.squeeze(0))\n",
    "\n",
    "print(f\"\\nMoE output sample:\")\n",
    "print(moe_output.squeeze(0))\n",
    "\n",
    "print(\"âœ“ Sparse MoE layer working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac10431d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Attention Mechanisms ===\n",
      "Single head input shape: torch.Size([2, 16, 64])\n",
      "Single head output shape: torch.Size([2, 16, 8])\n",
      "Multi-head attention output shape: torch.Size([2, 16, 64])\n",
      "MHA parameters: 16,448\n",
      "âœ“ Attention mechanisms working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Implement Attention Mechanisms\n",
    "class Head(nn.Module):\n",
    "    \"\"\"Single attention head for multi-head attention\"\"\"\n",
    "    def __init__(self, n_embed, head_size, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.block_size = block_size\n",
    "        \n",
    "        # Key, Query, Value projections\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False) \n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        \n",
    "        # Causal mask (lower triangular)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape  # batch, time, channels\n",
    "        \n",
    "        # Get key, query, value\n",
    "        k = self.key(x)   # (B, T, head_size)\n",
    "        q = self.query(x) # (B, T, head_size)\n",
    "        v = self.value(x) # (B, T, head_size)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        # Compute attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * (self.head_size ** -0.5)  # (B, T, T)\n",
    "        \n",
    "        # Apply causal mask (prevent looking at future tokens)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        \n",
    "        # Apply softmax\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        out = wei @ v  # (B, T, head_size)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head attention mechanism\"\"\"\n",
    "    def __init__(self, n_embed, n_head, block_size, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert n_embed % n_head == 0, \"n_embed must be divisible by n_head\"\n",
    "        \n",
    "        head_size = n_embed // n_head\n",
    "        self.heads = nn.ModuleList([\n",
    "            Head(n_embed, head_size, block_size, dropout) \n",
    "            for _ in range(n_head)\n",
    "        ])\n",
    "        \n",
    "        # Output projection\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Concatenate outputs from all heads\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        \n",
    "        # Apply output projection and dropout\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "# Test attention mechanisms\n",
    "print(\"=== Testing Attention Mechanisms ===\")\n",
    "n_embed = 64\n",
    "n_head = 8\n",
    "block_size = 32\n",
    "batch_size = 2\n",
    "seq_len = 16\n",
    "\n",
    "# Test single head\n",
    "head = Head(n_embed, n_embed // n_head, block_size)\n",
    "test_input = torch.randn(batch_size, seq_len, n_embed)\n",
    "head_output = head(test_input)\n",
    "\n",
    "print(f\"Single head input shape: {test_input.shape}\")\n",
    "print(f\"Single head output shape: {head_output.shape}\")\n",
    "\n",
    "# Test multi-head attention\n",
    "mha = MultiHeadAttention(n_embed, n_head, block_size)\n",
    "mha_output = mha(test_input)\n",
    "\n",
    "print(f\"Multi-head attention output shape: {mha_output.shape}\")\n",
    "print(f\"MHA parameters: {sum(p.numel() for p in mha.parameters()):,}\")\n",
    "\n",
    "# Verify shape preservation\n",
    "assert test_input.shape == mha_output.shape, \"MHA should preserve input shape\"\n",
    "print(\"âœ“ Attention mechanisms working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b795a693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Transformer Block with MoE ===\n",
      "Input shape: torch.Size([2, 16, 64])\n",
      "Block output shape: torch.Size([2, 16, 64])\n",
      "Block parameters: 279,360\n",
      "Gradients computed: True\n",
      "âœ“ Transformer block with MoE working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Implement Transformer Block with MoE\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer block that replaces FFN with MoE layer\n",
    "    Architecture: LayerNorm -> MultiHeadAttention -> LayerNorm -> MoE -> Residual connections\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embed, n_head, block_size, num_experts, top_k, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.sa = MultiHeadAttention(n_embed, n_head, block_size, dropout)\n",
    "        \n",
    "        # MoE layer (replaces traditional FFN)\n",
    "        self.moe = SparseMoE(n_embed, num_experts, top_k, dropout, use_noise=True)\n",
    "        \n",
    "        # Layer normalizations\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # First sub-layer: Multi-head attention with residual connection\n",
    "        # Pre-norm: LayerNorm -> Attention -> Residual\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        \n",
    "        # Second sub-layer: MoE with residual connection  \n",
    "        # Pre-norm: LayerNorm -> MoE -> Residual\n",
    "        x = x + self.moe(self.ln2(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test transformer block\n",
    "print(\"=== Testing Transformer Block with MoE ===\")\n",
    "transformer_block = TransformerBlock(\n",
    "    n_embed=64,\n",
    "    n_head=8, \n",
    "    block_size=32,\n",
    "    num_experts=8,\n",
    "    top_k=2,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randn(2, 16, 64)  # (batch, seq_len, embed_dim)\n",
    "block_output = transformer_block(test_input)\n",
    "\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Block output shape: {block_output.shape}\")\n",
    "print(f\"Block parameters: {sum(p.numel() for p in transformer_block.parameters()):,}\")\n",
    "\n",
    "# Verify shape preservation\n",
    "assert test_input.shape == block_output.shape, \"Transformer block should preserve shape\"\n",
    "\n",
    "# Check for gradient flow\n",
    "loss = block_output.sum()\n",
    "loss.backward()\n",
    "\n",
    "gradients_exist = any(p.grad is not None for p in transformer_block.parameters())\n",
    "print(f\"Gradients computed: {gradients_exist}\")\n",
    "\n",
    "print(\"âœ“ Transformer block with MoE working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45ac7c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Complete MoE Language Model ===\n",
      "Total model parameters: 1,128,001\n",
      "Input shape: torch.Size([4, 16])\n",
      "Logits shape: torch.Size([64, 65])\n",
      "Loss: 4.1748\n",
      "\n",
      "=== Testing Text Generation ===\n",
      "Generated text: 'b:RdQhkU!mmAOLa,&gbhW'\n",
      "âœ“ Complete MoE Language Model working correctly!\n",
      "Generated text: 'b:RdQhkU!mmAOLa,&gbhW'\n",
      "âœ“ Complete MoE Language Model working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Complete MoE Language Model\n",
    "class MoELanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete language model with Mixture of Experts\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, n_embed, n_head, n_layer, block_size, \n",
    "                 num_experts, top_k, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Input embeddings\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        \n",
    "        # Transformer blocks with MoE\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            TransformerBlock(n_embed, n_head, block_size, num_experts, top_k, dropout)\n",
    "            for _ in range(n_layer)\n",
    "        ])\n",
    "        \n",
    "        # Output layers\n",
    "        self.ln_f = nn.LayerNorm(n_embed)  # Final layer norm\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)  # Language modeling head\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize weights using Xavier/Glorot initialization\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        # Input embeddings\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B, T, n_embed)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device))  # (T, n_embed)\n",
    "        \n",
    "        # Combine token and position embeddings\n",
    "        x = tok_emb + pos_emb  # (B, T, n_embed)\n",
    "        \n",
    "        # Process through transformer blocks\n",
    "        x = self.blocks(x)  # (B, T, n_embed)\n",
    "        \n",
    "        # Final layer norm and output projection\n",
    "        x = self.ln_f(x)  # (B, T, n_embed)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "        \n",
    "        # Calculate loss if targets are provided\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "            \n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0):\n",
    "        \"\"\"Generate new tokens using the trained model\"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop context to block_size\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            \n",
    "            # Get predictions\n",
    "            logits, _ = self(idx_cond)\n",
    "            \n",
    "            # Focus on the last time step and apply temperature\n",
    "            logits = logits[:, -1, :] / temperature  # (B, C)\n",
    "            \n",
    "            # Sample from distribution\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            \n",
    "            # Append to sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "            \n",
    "        return idx\n",
    "\n",
    "# Test the complete model\n",
    "print(\"=== Testing Complete MoE Language Model ===\")\n",
    "\n",
    "# Model hyperparameters\n",
    "model_config = {\n",
    "    'vocab_size': vocab_size,  # From dataset preparation\n",
    "    'n_embed': 64,\n",
    "    'n_head': 8,\n",
    "    'n_layer': 4,\n",
    "    'block_size': 32,\n",
    "    'num_experts': 8,\n",
    "    'top_k': 2,\n",
    "    'dropout': 0.1\n",
    "}\n",
    "\n",
    "# Create model\n",
    "model = MoELanguageModel(**model_config)\n",
    "\n",
    "# Move to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total model parameters: {total_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "batch_size = 4\n",
    "seq_len = 16\n",
    "test_idx = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)\n",
    "test_targets = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits, loss = model(test_idx, test_targets)\n",
    "\n",
    "print(f\"Input shape: {test_idx.shape}\")\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Test generation\n",
    "print(\"\\n=== Testing Text Generation ===\")\n",
    "with torch.no_grad():\n",
    "    # Start with a random token\n",
    "    start_idx = torch.randint(0, vocab_size, (1, 1), device=device)\n",
    "    generated = model.generate(start_idx, max_new_tokens=20)\n",
    "    generated_text = decode(generated[0].cpu().tolist())\n",
    "    print(f\"Generated text: '{generated_text}'\")\n",
    "\n",
    "print(\"âœ“ Complete MoE Language Model working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fb71156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Preparing Training Data ===\n",
      "Training data size: 1,003,854 tokens\n",
      "Validation data size: 111,540 tokens\n",
      "Batch input shape: torch.Size([4, 32])\n",
      "Batch target shape: torch.Size([4, 32])\n",
      "\n",
      "Example input sequence:\n",
      "'\n",
      "What is't, knave?\n",
      "\n",
      "Servant:\n",
      "An '\n",
      "\n",
      "Corresponding target sequence:\n",
      "'What is't, knave?\n",
      "\n",
      "Servant:\n",
      "An h'\n",
      "\n",
      "Verifying input-target relationship:\n",
      "Position 0: input='\n",
      "' -> target='W' (matches next input='W')\n",
      "Position 1: input='W' -> target='h' (matches next input='h')\n",
      "Position 2: input='h' -> target='a' (matches next input='a')\n",
      "Position 3: input='a' -> target='t' (matches next input='t')\n",
      "Position 4: input='t' -> target=' ' (matches next input=' ')\n",
      "Position 5: input=' ' -> target='i' (matches next input='i')\n",
      "Position 6: input='i' -> target='s' (matches next input='s')\n",
      "Position 7: input='s' -> target=''' (matches next input=''')\n",
      "Position 8: input=''' -> target='t' (matches next input='t')\n",
      "Position 9: input='t' -> target=',' (matches next input=',')\n",
      "âœ“ Data preparation working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Step 10: Data Preparation and Batch Creation\n",
    "def create_data_split(text, train_ratio=0.9):\n",
    "    \"\"\"Split data into training and validation sets\"\"\"\n",
    "    data = torch.tensor(encode(text), dtype=torch.long)\n",
    "    n = int(train_ratio * len(data))\n",
    "    train_data = data[:n]\n",
    "    val_data = data[n:]\n",
    "    return train_data, val_data\n",
    "\n",
    "def get_batch(data, batch_size, block_size, device):\n",
    "    \"\"\"Generate a batch of input-target pairs\"\"\"\n",
    "    # Randomly select starting positions\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    \n",
    "    # Create input sequences\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    \n",
    "    # Create target sequences (shifted by 1)\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    \n",
    "    return x.to(device), y.to(device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model, train_data, val_data, batch_size, block_size, eval_iters=100):\n",
    "    \"\"\"Estimate training and validation loss\"\"\"\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    \n",
    "    for split, data in [('train', train_data), ('val', val_data)]:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(data, batch_size, block_size, device)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    \n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# Prepare data\n",
    "print(\"=== Preparing Training Data ===\")\n",
    "train_data, val_data = create_data_split(text, train_ratio=0.9)\n",
    "\n",
    "print(f\"Training data size: {len(train_data):,} tokens\")\n",
    "print(f\"Validation data size: {len(val_data):,} tokens\")\n",
    "\n",
    "# Test batch creation\n",
    "batch_size = 4\n",
    "block_size = 32\n",
    "\n",
    "X_batch, Y_batch = get_batch(train_data, batch_size, block_size, device)\n",
    "print(f\"Batch input shape: {X_batch.shape}\")\n",
    "print(f\"Batch target shape: {Y_batch.shape}\")\n",
    "\n",
    "# Show example input-target pair\n",
    "print(f\"\\nExample input sequence:\")\n",
    "print(f\"'{decode(X_batch[0].cpu().tolist())}'\")\n",
    "print(f\"\\nCorresponding target sequence:\")\n",
    "print(f\"'{decode(Y_batch[0].cpu().tolist())}'\")\n",
    "\n",
    "# Verify that target is input shifted by 1\n",
    "print(\"\\nVerifying input-target relationship:\")\n",
    "for i in range(min(10, block_size)):\n",
    "    input_char = decode([X_batch[0][i].item()])\n",
    "    target_char = decode([Y_batch[0][i].item()])\n",
    "    next_input_char = decode([X_batch[0][i+1].item()]) if i+1 < block_size else \"END\"\n",
    "    print(f\"Position {i}: input='{input_char}' -> target='{target_char}' (matches next input='{next_input_char}')\")\n",
    "\n",
    "print(\"âœ“ Data preparation working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f6fb84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Training Configuration ===\n",
      "Model Architecture:\n",
      "  - Vocabulary size: 65\n",
      "  - Embedding dimension: 64\n",
      "  - Number of heads: 8\n",
      "  - Number of layers: 4\n",
      "  - Block size: 32\n",
      "  - Number of experts: 8\n",
      "  - Top-K experts: 2\n",
      "  - Dropout: 0.1\n",
      "\n",
      "Training Parameters:\n",
      "  - Batch size: 16\n",
      "  - Learning rate: 0.001\n",
      "  - Max iterations: 1000\n",
      "  - Evaluation interval: 100\n",
      "  - Device: cpu\n",
      "\n",
      "Model Statistics:\n",
      "  - Total parameters: 1,128,001\n",
      "  - Model size: 4.5 MB (float32)\n",
      "âœ“ Training configuration ready!\n",
      "âœ“ Training configuration ready!\n"
     ]
    }
   ],
   "source": [
    "# Step 11: Training Configuration and Hyperparameters\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    # Model hyperparameters\n",
    "    vocab_size: int = vocab_size\n",
    "    n_embed: int = 64\n",
    "    n_head: int = 8\n",
    "    n_layer: int = 4\n",
    "    block_size: int = 32\n",
    "    num_experts: int = 8\n",
    "    top_k: int = 2\n",
    "    dropout: float = 0.1\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    batch_size: int = 16\n",
    "    learning_rate: float = 1e-3\n",
    "    max_iters: int = 1000  # Increase to 50000+ for better results\n",
    "    eval_interval: int = 100\n",
    "    eval_iters: int = 50\n",
    "    \n",
    "    # Hardware\n",
    "    device: str = device\n",
    "\n",
    "# Create training configuration\n",
    "config = TrainingConfig()\n",
    "\n",
    "print(\"=== Training Configuration ===\")\n",
    "print(f\"Model Architecture:\")\n",
    "print(f\"  - Vocabulary size: {config.vocab_size}\")\n",
    "print(f\"  - Embedding dimension: {config.n_embed}\")\n",
    "print(f\"  - Number of heads: {config.n_head}\")\n",
    "print(f\"  - Number of layers: {config.n_layer}\")\n",
    "print(f\"  - Block size: {config.block_size}\")\n",
    "print(f\"  - Number of experts: {config.num_experts}\")\n",
    "print(f\"  - Top-K experts: {config.top_k}\")\n",
    "print(f\"  - Dropout: {config.dropout}\")\n",
    "\n",
    "print(f\"\\nTraining Parameters:\")\n",
    "print(f\"  - Batch size: {config.batch_size}\")\n",
    "print(f\"  - Learning rate: {config.learning_rate}\")\n",
    "print(f\"  - Max iterations: {config.max_iters}\")\n",
    "print(f\"  - Evaluation interval: {config.eval_interval}\")\n",
    "print(f\"  - Device: {config.device}\")\n",
    "\n",
    "# Create and initialize model\n",
    "model = MoELanguageModel(\n",
    "    vocab_size=config.vocab_size,\n",
    "    n_embed=config.n_embed,\n",
    "    n_head=config.n_head,\n",
    "    n_layer=config.n_layer,\n",
    "    block_size=config.block_size,\n",
    "    num_experts=config.num_experts,\n",
    "    top_k=config.top_k,\n",
    "    dropout=config.dropout\n",
    ").to(config.device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nModel Statistics:\")\n",
    "print(f\"  - Total parameters: {total_params:,}\")\n",
    "print(f\"  - Model size: {total_params * 4 / 1e6:.1f} MB (float32)\")\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "print(\"âœ“ Training configuration ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6e86a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting MoE training...\n",
      "Training for 1000 iterations...\n",
      "=== Starting Training ===\n",
      "Step    0 | Train Loss: 4.1660 | Val Loss: 4.1661 | Time: 3.3s\n",
      "Step    0 | Train Loss: 4.1660 | Val Loss: 4.1661 | Time: 3.3s\n",
      "Step  100 | Train Loss: 2.6180 | Val Loss: 2.6337 | Time: 21.0s\n",
      "Step  100 | Train Loss: 2.6180 | Val Loss: 2.6337 | Time: 21.0s\n",
      "Step  200 | Train Loss: 2.4624 | Val Loss: 2.4869 | Time: 38.5s\n",
      "Step  200 | Train Loss: 2.4624 | Val Loss: 2.4869 | Time: 38.5s\n",
      "Step  300 | Train Loss: 2.4151 | Val Loss: 2.4216 | Time: 76.6s\n",
      "Step  300 | Train Loss: 2.4151 | Val Loss: 2.4216 | Time: 76.6s\n",
      "Step  400 | Train Loss: 2.3164 | Val Loss: 2.3282 | Time: 123.0s\n",
      "Step  400 | Train Loss: 2.3164 | Val Loss: 2.3282 | Time: 123.0s\n",
      "Step  500 | Train Loss: 2.2516 | Val Loss: 2.2752 | Time: 196.5s\n",
      "Step  500 | Train Loss: 2.2516 | Val Loss: 2.2752 | Time: 196.5s\n",
      "Step  600 | Train Loss: 2.1812 | Val Loss: 2.2254 | Time: 277.0s\n",
      "Step  600 | Train Loss: 2.1812 | Val Loss: 2.2254 | Time: 277.0s\n",
      "Step  700 | Train Loss: 2.1446 | Val Loss: 2.1672 | Time: 359.0s\n",
      "Step  700 | Train Loss: 2.1446 | Val Loss: 2.1672 | Time: 359.0s\n",
      "Step  800 | Train Loss: 2.1041 | Val Loss: 2.1276 | Time: 433.5s\n",
      "Step  800 | Train Loss: 2.1041 | Val Loss: 2.1276 | Time: 433.5s\n",
      "Step  900 | Train Loss: 2.0535 | Val Loss: 2.1102 | Time: 503.6s\n",
      "Step  900 | Train Loss: 2.0535 | Val Loss: 2.1102 | Time: 503.6s\n",
      "Step  999 | Train Loss: 2.0183 | Val Loss: 2.0664 | Time: 571.0s\n",
      "Step  999 | Train Loss: 2.0183 | Val Loss: 2.0664 | Time: 571.0s\n",
      "\n",
      "Training completed in 571.5s\n",
      "Final train loss: 2.0183\n",
      "Final val loss: 2.0664\n",
      "âœ… Training completed successfully!\n",
      "\n",
      "Training completed in 571.5s\n",
      "Final train loss: 2.0183\n",
      "Final val loss: 2.0664\n",
      "âœ… Training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Step 12: Training Loop\n",
    "import time\n",
    "\n",
    "def train_model(model, train_data, val_data, config, optimizer):\n",
    "    \"\"\"Complete training loop for MoE model\"\"\"\n",
    "    \n",
    "    print(\"=== Starting Training ===\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Training metrics\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for iter_num in range(config.max_iters):\n",
    "        # Evaluate model periodically\n",
    "        if iter_num % config.eval_interval == 0 or iter_num == config.max_iters - 1:\n",
    "            losses = estimate_loss(\n",
    "                model, train_data, val_data, \n",
    "                config.batch_size, config.block_size, config.eval_iters\n",
    "            )\n",
    "            \n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"Step {iter_num:4d} | \"\n",
    "                  f\"Train Loss: {losses['train']:.4f} | \"\n",
    "                  f\"Val Loss: {losses['val']:.4f} | \"\n",
    "                  f\"Time: {elapsed_time:.1f}s\")\n",
    "            \n",
    "            train_losses.append(losses['train'])\n",
    "            val_losses.append(losses['val'])\n",
    "        \n",
    "        # Get training batch\n",
    "        xb, yb = get_batch(train_data, config.batch_size, config.block_size, config.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, loss = model(xb, yb)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\nTraining completed in {total_time:.1f}s\")\n",
    "    print(f\"Final train loss: {train_losses[-1]:.4f}\")\n",
    "    print(f\"Final val loss: {val_losses[-1]:.4f}\")\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Start training\n",
    "print(\"ðŸš€ Starting MoE training...\")\n",
    "print(f\"Training for {config.max_iters} iterations...\")\n",
    "\n",
    "train_losses, val_losses = train_model(model, train_data, val_data, config, optimizer)\n",
    "\n",
    "print(\"âœ… Training completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09d0d5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Text Generation ===\n",
      "1. Random start, low temperature (focused):\n",
      "Generated: 'for the duve will my not lods not! Yord these of to the seee!\n",
      "\n",
      "\n",
      "FOpet bee the the beor of bear and the theve mugned andle in and that's I mim not that '\n",
      "\n",
      "==================================================\n",
      "2. Random start, high temperature (creative):\n",
      "Generated: 'for the duve will my not lods not! Yord these of to the seee!\n",
      "\n",
      "\n",
      "FOpet bee the the beor of bear and the theve mugned andle in and that's I mim not that '\n",
      "\n",
      "==================================================\n",
      "2. Random start, high temperature (creative):\n",
      "Generated: 'do-radelfuf thuu, siousy ow ace SI me alle ow\n",
      "nlibevh y thybh shimentle there thard foay with 'll Sexpus'sse hall'd frive'd go earterh\n",
      "The hy livins, m'\n",
      "\n",
      "==================================================\n",
      "3. With prompt:\n",
      "Generated: 'do-radelfuf thuu, siousy ow ace SI me alle ow\n",
      "nlibevh y thybh shimentle there thard foay with 'll Sexpus'sse hall'd frive'd go earterh\n",
      "The hy livins, m'\n",
      "\n",
      "==================================================\n",
      "3. With prompt:\n",
      "Prompt: 'ROMEO:'\n",
      "Generated: 'ROMEO:\n",
      "Comade hencegod a then:\n",
      "We cuip the be be thard a hour of inising and the wom mese,ry weeet\n",
      "My to d'\n",
      "\n",
      "==================================================\n",
      "4. Different character:\n",
      "Prompt: 'ROMEO:'\n",
      "Generated: 'ROMEO:\n",
      "Comade hencegod a then:\n",
      "We cuip the be be thard a hour of inising and the wom mese,ry weeet\n",
      "My to d'\n",
      "\n",
      "==================================================\n",
      "4. Different character:\n",
      "Prompt: 'JULIET:'\n",
      "Generated: 'JULIET:\n",
      "Ben'd con:\n",
      "Hare med hour welll lionen wicks shy\n",
      "His, bene would be sive defore:\n",
      "My peet to herends '\n",
      "\n",
      "âœ… Text generation completed!\n",
      "Prompt: 'JULIET:'\n",
      "Generated: 'JULIET:\n",
      "Ben'd con:\n",
      "Hare med hour welll lionen wicks shy\n",
      "His, bene would be sive defore:\n",
      "My peet to herends '\n",
      "\n",
      "âœ… Text generation completed!\n"
     ]
    }
   ],
   "source": [
    "# Step 13: Inference and Text Generation\n",
    "def generate_text(model, prompt=\"\", max_length=100, temperature=1.0, top_k=None):\n",
    "    \"\"\"Generate text using the trained MoE model\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode prompt or start with random token\n",
    "    if prompt:\n",
    "        context = torch.tensor(encode(prompt), dtype=torch.long, device=device).unsqueeze(0)\n",
    "    else:\n",
    "        # Start with a random character\n",
    "        context = torch.randint(0, vocab_size, (1, 1), device=device)\n",
    "    \n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(context, max_length, temperature)\n",
    "    \n",
    "    # Decode and return\n",
    "    generated_text = decode(generated[0].cpu().tolist())\n",
    "    model.train()\n",
    "    return generated_text\n",
    "\n",
    "# Test text generation with different settings\n",
    "print(\"=== Testing Text Generation ===\")\n",
    "\n",
    "# Test 1: Random start, low temperature (more focused)\n",
    "print(\"1. Random start, low temperature (focused):\")\n",
    "text1 = generate_text(model, prompt=\"\", max_length=150, temperature=0.8)\n",
    "print(f\"Generated: '{text1}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Test 2: Random start, high temperature (more creative)\n",
    "print(\"2. Random start, high temperature (creative):\")\n",
    "text2 = generate_text(model, prompt=\"\", max_length=150, temperature=1.2)\n",
    "print(f\"Generated: '{text2}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Test 3: With prompt\n",
    "print(\"3. With prompt:\")\n",
    "prompt = \"ROMEO:\"\n",
    "text3 = generate_text(model, prompt=prompt, max_length=100, temperature=1.0)\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"Generated: '{text3}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Test 4: Another character\n",
    "print(\"4. Different character:\")\n",
    "prompt = \"JULIET:\"\n",
    "text4 = generate_text(model, prompt=prompt, max_length=100, temperature=1.0)\n",
    "print(f\"Prompt: '{prompt}'\")\n",
    "print(f\"Generated: '{text4}'\")\n",
    "\n",
    "print(\"\\nâœ… Text generation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97345100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing expert utilization...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mixed dtype (CPU): all inputs must share same datatype.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 56\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Analyze expert usage\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalyzing expert utilization...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m expert_usage \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_expert_usage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m print_expert_analysis(expert_usage, config)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Model statistics\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 16\u001b[0m, in \u001b[0;36manalyze_expert_usage\u001b[1;34m(model, data, num_batches)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx, block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(model\u001b[38;5;241m.\u001b[39mblocks):\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(block, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmoe\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;66;03m# Get routing decisions\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m         routing_weights, expert_indices \u001b[38;5;241m=\u001b[39m block\u001b[38;5;241m.\u001b[39mmoe\u001b[38;5;241m.\u001b[39mrouter(\u001b[43mblock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;66;03m# Count expert usage\u001b[39;00m\n\u001b[0;32m     19\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m expert_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_experts):\n",
      "File \u001b[1;32mc:\\Users\\sayus\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sayus\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\sayus\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\normalization.py:217\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 217\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sayus\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\functional.py:2910\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[0;32m   2901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2902\u001b[0m         layer_norm,\n\u001b[0;32m   2903\u001b[0m         (\u001b[38;5;28minput\u001b[39m, weight, bias),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2908\u001b[0m         eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[0;32m   2909\u001b[0m     )\n\u001b[1;32m-> 2910\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2911\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[0;32m   2912\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mixed dtype (CPU): all inputs must share same datatype."
     ]
    }
   ],
   "source": [
    "# Step 14: Model Analysis and Expert Utilization\n",
    "def analyze_expert_usage(model, data, num_batches=10):\n",
    "    \"\"\"Analyze how experts are being utilized\"\"\"\n",
    "    model.eval()\n",
    "    expert_usage = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx in range(num_batches):\n",
    "            # Get batch\n",
    "            x, _ = get_batch(data, config.batch_size, config.block_size, config.device)\n",
    "            \n",
    "            # Ensure x is on the correct device and dtype\n",
    "            x = x.to(config.device)\n",
    "            \n",
    "            # Track expert usage in each MoE layer\n",
    "            for layer_idx, block in enumerate(model.blocks):\n",
    "                if hasattr(block, 'moe'):\n",
    "                    # Apply layer norm first, ensuring proper device/dtype\n",
    "                    normalized_x = block.ln2(x)\n",
    "                    \n",
    "                    # Get routing decisions\n",
    "                    routing_weights, expert_indices = block.moe.router(normalized_x)\n",
    "                    \n",
    "                    # Count expert usage\n",
    "                    for expert_id in range(config.num_experts):\n",
    "                        expert_mask = (expert_indices == expert_id)\n",
    "                        usage_count = expert_mask.sum().item()\n",
    "                        \n",
    "                        key = f\"layer_{layer_idx}_expert_{expert_id}\"\n",
    "                        if key not in expert_usage:\n",
    "                            expert_usage[key] = 0\n",
    "                        expert_usage[key] += usage_count\n",
    "    \n",
    "    model.train()\n",
    "    return expert_usage\n",
    "\n",
    "def print_expert_analysis(expert_usage, config):\n",
    "    \"\"\"Print expert utilization analysis\"\"\"\n",
    "    print(\"=== Expert Utilization Analysis ===\")\n",
    "    \n",
    "    for layer_idx in range(config.n_layer):\n",
    "        print(f\"\\nLayer {layer_idx}:\")\n",
    "        layer_usage = []\n",
    "        \n",
    "        for expert_id in range(config.num_experts):\n",
    "            key = f\"layer_{layer_idx}_expert_{expert_id}\"\n",
    "            usage = expert_usage.get(key, 0)\n",
    "            layer_usage.append(usage)\n",
    "            print(f\"  Expert {expert_id}: {usage:4d} tokens\")\n",
    "        \n",
    "        # Calculate balance metrics\n",
    "        total_usage = sum(layer_usage)\n",
    "        if total_usage > 0:\n",
    "            usage_percentages = [u/total_usage*100 for u in layer_usage]\n",
    "            std_dev = np.std(usage_percentages)\n",
    "            print(f\"  Balance (std dev): {std_dev:.2f}% (lower is better)\")\n",
    "            print(f\"  Most used expert: {np.argmax(layer_usage)} ({max(usage_percentages):.1f}%)\")\n",
    "            print(f\"  Least used expert: {np.argmin(layer_usage)} ({min(usage_percentages):.1f}%)\")\n",
    "\n",
    "# Analyze expert usage\n",
    "print(\"Analyzing expert utilization...\")\n",
    "expert_usage = analyze_expert_usage(model, val_data, num_batches=20)\n",
    "print_expert_analysis(expert_usage, config)\n",
    "\n",
    "# Model statistics\n",
    "print(f\"\\n=== Model Statistics ===\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Parameters by component\n",
    "attention_params = sum(p.numel() for block in model.blocks for p in block.sa.parameters())\n",
    "moe_params = sum(p.numel() for block in model.blocks for p in block.moe.parameters())\n",
    "embedding_params = sum(p.numel() for p in [model.token_embedding_table, model.position_embedding_table])\n",
    "output_params = sum(p.numel() for p in [model.ln_f, model.lm_head])\n",
    "\n",
    "print(f\"Attention parameters: {attention_params:,} ({attention_params/sum(p.numel() for p in model.parameters())*100:.1f}%)\")\n",
    "print(f\"MoE parameters: {moe_params:,} ({moe_params/sum(p.numel() for p in model.parameters())*100:.1f}%)\")\n",
    "print(f\"Embedding parameters: {embedding_params:,} ({embedding_params/sum(p.numel() for p in model.parameters())*100:.1f}%)\")\n",
    "print(f\"Output parameters: {output_params:,} ({output_params/sum(p.numel() for p in model.parameters())*100:.1f}%)\")\n",
    "\n",
    "# Active parameters (considering sparsity)\n",
    "active_experts_per_token = config.top_k\n",
    "expert_params_per_layer = moe_params // (config.n_layer * config.num_experts)\n",
    "active_moe_params = expert_params_per_layer * active_experts_per_token * config.n_layer\n",
    "active_total = attention_params + active_moe_params + embedding_params + output_params\n",
    "\n",
    "print(f\"\\nActive parameters per forward pass:\")\n",
    "print(f\"Active MoE parameters: {active_moe_params:,}\")\n",
    "print(f\"Total active parameters: {active_total:,}\")\n",
    "print(f\"Sparsity ratio: {active_total/sum(p.numel() for p in model.parameters())*100:.1f}%\")\n",
    "\n",
    "print(\"âœ… Model analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba653f98",
   "metadata": {},
   "source": [
    "# Step 15: Exercises and Extensions\n",
    "\n",
    "## ðŸš€ Congratulations!\n",
    "You've successfully implemented a complete Mixture of Experts model from scratch! This implementation includes:\n",
    "\n",
    "âœ… **Expert Networks**: Individual feed-forward neural networks  \n",
    "âœ… **Router Mechanism**: Top-K routing with load balancing  \n",
    "âœ… **Sparse MoE Layer**: Efficient expert combination  \n",
    "âœ… **Complete Transformer**: MoE-enabled transformer architecture  \n",
    "âœ… **Training Pipeline**: Full pre-training on Shakespeare dataset  \n",
    "âœ… **Inference System**: Text generation with trained model  \n",
    "âœ… **Analysis Tools**: Expert utilization metrics  \n",
    "\n",
    "## ðŸ”¬ Exercises to Try\n",
    "\n",
    "### 1. **Capacity Factor Implementation**\n",
    "Add expert capacity constraints to prevent overloading:\n",
    "```python\n",
    "# Implement capacity factor to limit tokens per expert\n",
    "def add_capacity_factor(self, capacity_factor=1.25):\n",
    "    # Calculate expert capacity based on total tokens and capacity factor\n",
    "    pass\n",
    "```\n",
    "\n",
    "### 2. **DeepSeek Auxiliary Loss-Free Load Balancing**\n",
    "Implement DeepSeek's bias-based load balancing:\n",
    "```python\n",
    "# Add dynamic bias terms to router\n",
    "class DeepSeekRouter(nn.Module):\n",
    "    def __init__(self, n_embed, num_experts, top_k, update_rate=0.1):\n",
    "        # Initialize bias terms\n",
    "        self.bias_terms = nn.Parameter(torch.zeros(num_experts))\n",
    "        # Implement bias update logic\n",
    "        pass\n",
    "```\n",
    "\n",
    "### 3. **Shared Experts Architecture**\n",
    "Add DeepSeek's shared experts alongside routed experts:\n",
    "```python\n",
    "# Implement shared + routed experts\n",
    "class SharedMoE(nn.Module):\n",
    "    def __init__(self, n_embed, num_shared, num_routed, top_k):\n",
    "        self.shared_experts = nn.ModuleList([...])  # Always active\n",
    "        self.routed_experts = nn.ModuleList([...])  # Selectively active\n",
    "        pass\n",
    "```\n",
    "\n",
    "### 4. **Fine-Grained Expert Segmentation**\n",
    "Implement more experts with smaller dimensions:\n",
    "```python\n",
    "# Increase expert count while maintaining parameter count\n",
    "def create_segmented_experts(base_experts, segmentation_factor):\n",
    "    # Split experts into smaller specialized experts\n",
    "    pass\n",
    "```\n",
    "\n",
    "### 5. **Load Balancing Loss**\n",
    "Add traditional auxiliary loss for comparison:\n",
    "```python\n",
    "# Implement Fi * Pi load balancing loss\n",
    "def calculate_load_balance_loss(routing_weights, expert_assignments):\n",
    "    # Calculate expert importance and token fractions\n",
    "    # Minimize Fi * Pi for better balance\n",
    "    pass\n",
    "```\n",
    "\n",
    "## ðŸŽ¯ Advanced Challenges\n",
    "\n",
    "### Performance Optimization\n",
    "- **Memory Efficiency**: Implement expert parallelization\n",
    "- **Speed Optimization**: Add CUDA kernels for routing\n",
    "- **Dynamic Routing**: Implement learned routing strategies\n",
    "\n",
    "### Architecture Innovations\n",
    "- **Hierarchical Experts**: Multi-level expert organization\n",
    "- **Mixture of Depths**: Variable computation per token\n",
    "- **Adaptive Sparsity**: Dynamic top-k selection\n",
    "\n",
    "### Training Improvements\n",
    "- **Curriculum Learning**: Progressive expert activation\n",
    "- **Expert Specialization**: Guided expert training\n",
    "- **Regularization**: Novel techniques for expert diversity\n",
    "\n",
    "## ðŸ“Š Experiment Ideas\n",
    "\n",
    "1. **Compare Architectures**: Dense vs Sparse MoE performance\n",
    "2. **Scaling Study**: Effect of expert count on quality\n",
    "3. **Load Balancing**: Compare different balancing strategies\n",
    "4. **Domain Adaptation**: Train experts on different text types\n",
    "5. **Efficiency Analysis**: Measure computational savings\n",
    "\n",
    "## ðŸ”§ Production Considerations\n",
    "\n",
    "When scaling this implementation:\n",
    "- **Distributed Training**: Multi-GPU expert placement\n",
    "- **Inference Optimization**: Expert caching strategies\n",
    "- **Model Serving**: Efficient expert loading\n",
    "- **Monitoring**: Expert utilization tracking\n",
    "\n",
    "## ðŸ“š Further Reading\n",
    "\n",
    "- **Original MoE Paper**: \"Outrageously Large Neural Networks\" (Shazeer et al.)\n",
    "- **Switch Transformer**: Improved MoE scaling (Fedus et al.)\n",
    "- **DeepSeek Papers**: V2 and V3 innovations\n",
    "- **GLaM**: Efficient MoE training (Du et al.)\n",
    "\n",
    "## ðŸŽ‰ Next Steps\n",
    "\n",
    "You now have a solid foundation in MoE architectures! Use this implementation to:\n",
    "- Research new routing mechanisms\n",
    "- Experiment with expert specialization\n",
    "- Scale to larger models and datasets\n",
    "- Contribute to the open-source community\n",
    "\n",
    "**Happy experimenting with Mixture of Experts!** ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
