{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5e94c69",
   "metadata": {},
   "source": [
    "# Multi-Token Prediction (MTP) Implementation from Scratch\n",
    "\n",
    "## Overview\n",
    "This notebook implements DeepSeek's Multi-Token Prediction mechanism from scratch. MTP is one of the three key innovations in DeepSeek V3 architecture, alongside Multi-Head Latent Attention (MLA) and Mixture of Experts (MoE).\n",
    "\n",
    "## What We'll Build\n",
    "1. **RMS Normalization Class**: For normalizing hidden states and embeddings\n",
    "2. **Multi-Token Prediction Class**: Complete MTP implementation with causal chains\n",
    "3. **Forward Pass**: Generate multiple future tokens for each input token\n",
    "4. **Loss Calculation**: Compute loss between predicted and target tokens\n",
    "\n",
    "## Key Concepts\n",
    "- **Prediction Depth (k)**: Number of future tokens to predict (e.g., k=3 means predict 3 tokens ahead)\n",
    "- **Causal Chain**: Each prediction head uses hidden state from previous head\n",
    "- **Input Requirements**: Hidden state + input embedding for each prediction head\n",
    "- **Output**: Multi-dimensional tensor containing predictions for all input positions and depths\n",
    "\n",
    "Let's implement this step by step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e080540f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0: Import Required Packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check device availability\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(\"‚úÖ Packages imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2dce22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define RMS Normalization Class\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Root Mean Square Normalization\n",
    "    \n",
    "    RMS normalization formula: x / sqrt(mean(x¬≤) + Œµ)\n",
    "    This differs from LayerNorm which uses: (x - mean(x)) / sqrt(var(x) + Œµ)\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Calculate RMS: sqrt(mean(x¬≤))\n",
    "        # x shape: (..., d_model)\n",
    "        \n",
    "        # Step 1: Square all elements\n",
    "        x_squared = x ** 2\n",
    "        \n",
    "        # Step 2: Take mean along the last dimension (d_model)\n",
    "        mean_squared = x_squared.mean(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Step 3: Take square root and add epsilon to prevent division by zero\n",
    "        rms = torch.sqrt(mean_squared + self.eps)\n",
    "        \n",
    "        # Step 4: Normalize by dividing each element by RMS\n",
    "        return x / rms\n",
    "\n",
    "# Test RMS Normalization\n",
    "print(\"=== Testing RMS Normalization ===\")\n",
    "d_model = 8\n",
    "rms_norm = RMSNorm(d_model)\n",
    "\n",
    "# Create test tensor\n",
    "test_tensor = torch.randn(2, 4, d_model)  # (batch_size, seq_len, d_model)\n",
    "print(f\"Input shape: {test_tensor.shape}\")\n",
    "print(f\"Input sample:\\n{test_tensor[0, 0, :]}\")\n",
    "\n",
    "# Apply RMS normalization\n",
    "normalized = rms_norm(test_tensor)\n",
    "print(f\"Output shape: {normalized.shape}\")\n",
    "print(f\"Output sample:\\n{normalized[0, 0, :]}\")\n",
    "\n",
    "# Verify normalization (RMS should be approximately 1.0)\n",
    "rms_value = torch.sqrt((normalized[0, 0, :] ** 2).mean())\n",
    "print(f\"RMS of normalized tensor: {rms_value:.6f} (should be ‚âà 1.0)\")\n",
    "\n",
    "print(\"‚úÖ RMS Normalization working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9ee6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Multi-Token Prediction Class (Main Implementation)\n",
    "class SimpleMTP(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Multi-Token Prediction implementation based on DeepSeek's approach\n",
    "    \n",
    "    Key Features:\n",
    "    - Predicts multiple tokens (depth k) for each input position\n",
    "    - Maintains causal chain between prediction heads\n",
    "    - Uses RMS normalization before merging hidden states and embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, vocab_size, num_heads, nhead=8, dim_feedforward=2048):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_heads = num_heads  # This is prediction depth, not attention heads\n",
    "        \n",
    "        # RMS normalization layers\n",
    "        self.rms_norm = RMSNorm(d_model)\n",
    "        \n",
    "        # Projection layers: (2*d_model) -> d_model\n",
    "        # We concatenate hidden_state (d_model) + input_embedding (d_model) = 2*d_model\n",
    "        self.projections = nn.Linear(2 * d_model, d_model)\n",
    "        \n",
    "        # Transformer encoder layer for processing merged embeddings\n",
    "        self.transformer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            batch_first=True,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        # Shared unembedding matrix: d_model -> vocab_size\n",
    "        self.unembedding = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        # Token embeddings\n",
    "        self.embeddings = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "    def forward(self, input_tokens):\n",
    "        \"\"\"\n",
    "        Forward pass for Multi-Token Prediction\n",
    "        \n",
    "        Args:\n",
    "            input_tokens: (batch_size, seq_len) - Input token indices\n",
    "            \n",
    "        Returns:\n",
    "            logits: (batch_size, max_i, num_heads, vocab_size) - Predictions for each position and depth\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = input_tokens.shape\n",
    "        \n",
    "        # Get token embeddings\n",
    "        embeds = self.embeddings(input_tokens)  # (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # Calculate maximum input position we can predict from\n",
    "        # We need 'num_heads' future positions to exist\n",
    "        max_i = seq_len - self.num_heads\n",
    "        \n",
    "        # Initialize output list to store predictions\n",
    "        all_predictions = []\n",
    "        \n",
    "        # Outer loop: Iterate over input token positions (i = 0, 1, 2, ..., max_i-1)\n",
    "        for i in range(max_i):\n",
    "            \n",
    "            # Initialize h_previous for this input position\n",
    "            # h_previous starts as the embedding of the current token\n",
    "            h_previous = embeds[:, i:i+1, :]  # (batch_size, 1, d_model)\n",
    "            \n",
    "            # Store predictions for this input position across all depths\n",
    "            position_predictions = []\n",
    "            \n",
    "            # Inner loop: Iterate over prediction depths (k = 0, 1, 2, ..., num_heads-1)\n",
    "            for k in range(self.num_heads):\n",
    "                \n",
    "                # Calculate future position we're predicting\n",
    "                future_pos = i + k + 1  # +1 because Python is 0-indexed\n",
    "                \n",
    "                # Get input embedding at the future position\n",
    "                token_embedding = embeds[:, future_pos:future_pos+1, :]  # (batch_size, 1, d_model)\n",
    "                \n",
    "                # === HEAD OPERATIONS ===\n",
    "                \n",
    "                # Step 1: RMS Normalization of both inputs\n",
    "                h_norm = self.rms_norm(h_previous)      # (batch_size, 1, d_model)\n",
    "                e_norm = self.rms_norm(token_embedding)  # (batch_size, 1, d_model)\n",
    "                \n",
    "                # Step 2: Merge (concatenate) normalized hidden state and embedding\n",
    "                merged = torch.cat([h_norm, e_norm], dim=-1)  # (batch_size, 1, 2*d_model)\n",
    "                \n",
    "                # Step 3: Linear projection back to d_model\n",
    "                projected = self.projections(merged)  # (batch_size, 1, d_model)\n",
    "                \n",
    "                # Step 4: Pass through transformer block\n",
    "                h_current = self.transformer(projected)  # (batch_size, 1, d_model)\n",
    "                \n",
    "                # Step 5: Generate logits using shared unembedding matrix\n",
    "                logits_k = self.unembedding(h_current)  # (batch_size, 1, vocab_size)\n",
    "                \n",
    "                # Store prediction for this depth\n",
    "                position_predictions.append(logits_k)\n",
    "                \n",
    "                # Update h_previous for next iteration (causal chain)\n",
    "                h_previous = h_current\n",
    "            \n",
    "            # Stack predictions for this position across all depths\n",
    "            # From list of (batch_size, 1, vocab_size) to (batch_size, num_heads, vocab_size)\n",
    "            position_logits = torch.cat(position_predictions, dim=1)  # (batch_size, num_heads, vocab_size)\n",
    "            all_predictions.append(position_logits)\n",
    "        \n",
    "        # Stack all position predictions\n",
    "        # From list of (batch_size, num_heads, vocab_size) to (batch_size, max_i, num_heads, vocab_size)\n",
    "        final_logits = torch.stack(all_predictions, dim=1)  # (batch_size, max_i, num_heads, vocab_size)\n",
    "        \n",
    "        return final_logits\n",
    "\n",
    "print(\"‚úÖ SimpleMTP class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51763691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Generate Next Tokens (Testing the Model)\n",
    "\n",
    "# Define model hyperparameters\n",
    "batch_size = 1          # Number of sequences to process\n",
    "seq_len = 8             # Length of input sequence (T)\n",
    "d_model = 8             # Embedding dimension\n",
    "vocab_size = 5000       # Vocabulary size\n",
    "num_heads = 3           # Prediction depth (number of future tokens to predict)\n",
    "\n",
    "print(\"=== Model Configuration ===\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Sequence length: {seq_len}\")\n",
    "print(f\"Model dimension: {d_model}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Prediction depth: {num_heads}\")\n",
    "print(f\"Predictable positions: {seq_len - num_heads} (positions 0 to {seq_len - num_heads - 1})\")\n",
    "\n",
    "# Create model instance\n",
    "model = SimpleMTP(\n",
    "    d_model=d_model,\n",
    "    vocab_size=vocab_size,\n",
    "    num_heads=num_heads,\n",
    "    nhead=4,  # Number of attention heads in transformer\n",
    "    dim_feedforward=32\n",
    ")\n",
    "\n",
    "# Move to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nModel parameters: {total_params:,}\")\n",
    "\n",
    "# Create batch of input tokens (randomly sampled)\n",
    "input_tokens = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)\n",
    "print(f\"\\nInput tokens shape: {input_tokens.shape}\")\n",
    "print(f\"Sample input tokens: {input_tokens[0].tolist()}\")\n",
    "\n",
    "# Forward pass through model\n",
    "with torch.no_grad():\n",
    "    logits = model(input_tokens)\n",
    "\n",
    "print(f\"\\n=== Output Analysis ===\")\n",
    "print(f\"Output logits shape: {logits.shape}\")\n",
    "print(f\"Expected shape: (batch_size={batch_size}, max_i={seq_len-num_heads}, num_heads={num_heads}, vocab_size={vocab_size})\")\n",
    "\n",
    "# Analyze output dimensions\n",
    "batch_dim, max_i_dim, heads_dim, vocab_dim = logits.shape\n",
    "print(f\"\\nDimension breakdown:\")\n",
    "print(f\"  Batch dimension: {batch_dim} (batch_size)\")\n",
    "print(f\"  Position dimension: {max_i_dim} (T - D = {seq_len} - {num_heads} = {seq_len-num_heads})\")\n",
    "print(f\"  Heads dimension: {heads_dim} (prediction depth)\")\n",
    "print(f\"  Vocabulary dimension: {vocab_dim} (vocab_size)\")\n",
    "\n",
    "# Test specific predictions\n",
    "print(f\"\\n=== Prediction Examples ===\")\n",
    "\n",
    "# Example 1: Predictions for input position i=0\n",
    "print(f\"1. Predictions for input position i=0:\")\n",
    "i0_logits = logits[0, 0, :, :]  # Shape: (num_heads, vocab_size)\n",
    "i0_predictions = torch.argmax(i0_logits, dim=-1)  # Shape: (num_heads,)\n",
    "print(f\"   Predicted tokens: {i0_predictions.tolist()}\")\n",
    "print(f\"   Shape: {i0_predictions.shape}\")\n",
    "\n",
    "# Example 2: Predictions for first head (k=0) across all positions\n",
    "print(f\"\\n2. Predictions for head k=0 across all positions:\")\n",
    "k0_logits = logits[0, :, 0, :]  # Shape: (max_i, vocab_size)\n",
    "k0_predictions = torch.argmax(k0_logits, dim=-1)  # Shape: (max_i,)\n",
    "print(f\"   Predicted tokens: {k0_predictions.tolist()}\")\n",
    "print(f\"   Shape: {k0_predictions.shape}\")\n",
    "\n",
    "# Example 3: Single prediction (i=0, k=0)\n",
    "print(f\"\\n3. Single prediction for i=0, k=0:\")\n",
    "single_logits = logits[0, 0, 0, :]  # Shape: (vocab_size,)\n",
    "single_prediction = torch.argmax(single_logits)\n",
    "print(f\"   Predicted token: {single_prediction.item()}\")\n",
    "print(f\"   Logits shape: {single_logits.shape}\")\n",
    "\n",
    "print(\"\\n‚úÖ Model forward pass completed successfully!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0f4bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Loss Function Calculation\n",
    "\n",
    "def compute_mtp_loss(logits, input_tokens):\n",
    "    \"\"\"\n",
    "    Compute Multi-Token Prediction loss\n",
    "    \n",
    "    Args:\n",
    "        logits: (batch_size, max_i, num_heads, vocab_size) - Model predictions\n",
    "        input_tokens: (batch_size, seq_len) - Original input tokens\n",
    "    \n",
    "    Returns:\n",
    "        loss: Scalar tensor representing average loss across all predictions\n",
    "    \"\"\"\n",
    "    batch_size, max_i, num_heads, vocab_size = logits.shape\n",
    "    total_loss = 0.0\n",
    "    total_predictions = 0\n",
    "    \n",
    "    # Iterate over input positions\n",
    "    for i in range(max_i):\n",
    "        \n",
    "        # Iterate over prediction depths for this position\n",
    "        for k in range(num_heads):\n",
    "            \n",
    "            # Get predicted logits for position i, depth k\n",
    "            predicted_logits = logits[:, i, k, :]  # (batch_size, vocab_size)\n",
    "            \n",
    "            # Get target token at position i + k + 1 (future position)\n",
    "            target_pos = i + k + 1\n",
    "            target_tokens = input_tokens[:, target_pos]  # (batch_size,)\n",
    "            \n",
    "            # Compute cross-entropy loss for this prediction\n",
    "            loss_ik = F.cross_entropy(predicted_logits, target_tokens)\n",
    "            \n",
    "            # Add to total loss\n",
    "            total_loss += loss_ik\n",
    "            total_predictions += 1\n",
    "    \n",
    "    # Return average loss\n",
    "    return total_loss / total_predictions\n",
    "\n",
    "# Test loss calculation\n",
    "print(\"=== Loss Function Calculation ===\")\n",
    "\n",
    "# Compute loss\n",
    "loss = compute_mtp_loss(logits, input_tokens)\n",
    "print(f\"Total MTP Loss: {loss.item():.6f}\")\n",
    "\n",
    "# Break down loss calculation for understanding\n",
    "batch_size, max_i, num_heads, vocab_size = logits.shape\n",
    "print(f\"\\nLoss breakdown:\")\n",
    "print(f\"  Total positions: {max_i}\")\n",
    "print(f\"  Predictions per position: {num_heads}\")\n",
    "print(f\"  Total predictions: {max_i * num_heads}\")\n",
    "print(f\"  Average loss per prediction: {loss.item():.6f}\")\n",
    "\n",
    "# Detailed loss analysis for first few predictions\n",
    "print(f\"\\n=== Detailed Loss Analysis ===\")\n",
    "individual_losses = []\n",
    "\n",
    "for i in range(min(3, max_i)):  # Show first 3 positions\n",
    "    print(f\"\\nPosition i={i}:\")\n",
    "    \n",
    "    for k in range(num_heads):\n",
    "        # Get prediction and target\n",
    "        predicted_logits = logits[0, i, k, :]  # (vocab_size,)\n",
    "        target_pos = i + k + 1\n",
    "        target_token = input_tokens[0, target_pos].item()\n",
    "        \n",
    "        # Compute loss for this specific prediction\n",
    "        loss_ik = F.cross_entropy(predicted_logits.unsqueeze(0), \n",
    "                                 input_tokens[0, target_pos:target_pos+1])\n",
    "        individual_losses.append(loss_ik.item())\n",
    "        \n",
    "        # Get predicted token\n",
    "        predicted_token = torch.argmax(predicted_logits).item()\n",
    "        \n",
    "        print(f\"  Depth k={k}: Target={target_token}, Predicted={predicted_token}, Loss={loss_ik.item():.4f}\")\n",
    "\n",
    "print(f\"\\nIndividual losses: {[f'{l:.4f}' for l in individual_losses]}\")\n",
    "print(f\"Mean of individual losses: {sum(individual_losses) / len(individual_losses):.6f}\")\n",
    "\n",
    "# Demonstrate backpropagation\n",
    "print(f\"\\n=== Backpropagation Demo ===\")\n",
    "model.train()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Forward pass\n",
    "logits = model(input_tokens)\n",
    "loss = compute_mtp_loss(logits, input_tokens)\n",
    "\n",
    "print(f\"Loss before backprop: {loss.item():.6f}\")\n",
    "\n",
    "# Backward pass\n",
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "# Check gradients\n",
    "total_grad_norm = 0.0\n",
    "for name, param in model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        grad_norm = param.grad.norm().item()\n",
    "        total_grad_norm += grad_norm ** 2\n",
    "\n",
    "total_grad_norm = total_grad_norm ** 0.5\n",
    "print(f\"Total gradient norm: {total_grad_norm:.6f}\")\n",
    "\n",
    "# Update parameters\n",
    "optimizer.step()\n",
    "\n",
    "# Forward pass after update\n",
    "with torch.no_grad():\n",
    "    new_logits = model(input_tokens)\n",
    "    new_loss = compute_mtp_loss(new_logits, input_tokens)\n",
    "\n",
    "print(f\"Loss after one step: {new_loss.item():.6f}\")\n",
    "print(f\"Loss change: {new_loss.item() - loss.item():.6f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Loss calculation and backpropagation working correctly!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83972bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization and Understanding\n",
    "\n",
    "def visualize_mtp_predictions(input_tokens, logits, max_examples=3):\n",
    "    \"\"\"\n",
    "    Visualize Multi-Token Prediction results in a clear format\n",
    "    \"\"\"\n",
    "    print(\"=== Multi-Token Prediction Visualization ===\")\n",
    "    \n",
    "    batch_size, max_i, num_heads, vocab_size = logits.shape\n",
    "    seq_len = input_tokens.shape[1]\n",
    "    \n",
    "    print(f\"Input sequence: {input_tokens[0].tolist()}\")\n",
    "    print(f\"Sequence length: {seq_len}\")\n",
    "    print(f\"Prediction depth: {num_heads}\")\n",
    "    print(f\"Predictable positions: {max_i} (positions 0 to {max_i-1})\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{'Input Pos':<10} {'Predictions':<50} {'Targets':<20}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    for i in range(min(max_examples, max_i)):\n",
    "        # Get predictions for this position\n",
    "        position_logits = logits[0, i, :, :]  # (num_heads, vocab_size)\n",
    "        predictions = torch.argmax(position_logits, dim=-1)  # (num_heads,)\n",
    "        \n",
    "        # Get target tokens\n",
    "        targets = []\n",
    "        for k in range(num_heads):\n",
    "            target_pos = i + k + 1\n",
    "            targets.append(input_tokens[0, target_pos].item())\n",
    "        \n",
    "        # Format output\n",
    "        pred_str = f\"{predictions.tolist()}\"\n",
    "        target_str = f\"{targets}\"\n",
    "        \n",
    "        print(f\"i={i:<8} {pred_str:<50} {target_str:<20}\")\n",
    "        \n",
    "        # Show which future positions these correspond to\n",
    "        future_positions = [i + k + 1 for k in range(num_heads)]\n",
    "        print(f\"{'':>10} Future positions: {future_positions}\")\n",
    "        print()\n",
    "\n",
    "# Run visualization\n",
    "visualize_mtp_predictions(input_tokens, logits)\n",
    "\n",
    "print(\"=== Understanding the Causal Chain ===\")\n",
    "print(\"\"\"\n",
    "The key innovation in DeepSeek's MTP is the causal chain between prediction heads:\n",
    "\n",
    "1. **Head 1 (k=0)**: \n",
    "   - Input: h‚ÇÄ (from transformer) + embedding at position i+1\n",
    "   - Output: hidden_state‚ÇÅ + prediction‚ÇÅ\n",
    "\n",
    "2. **Head 2 (k=1)**:\n",
    "   - Input: hidden_state‚ÇÅ (from Head 1) + embedding at position i+2  \n",
    "   - Output: hidden_state‚ÇÇ + prediction‚ÇÇ\n",
    "\n",
    "3. **Head 3 (k=2)**:\n",
    "   - Input: hidden_state‚ÇÇ (from Head 2) + embedding at position i+3\n",
    "   - Output: hidden_state‚ÇÉ + prediction‚ÇÉ\n",
    "\n",
    "This creates dependencies: prediction‚ÇÇ depends on prediction‚ÇÅ, prediction‚ÇÉ depends on prediction‚ÇÇ.\n",
    "This is different from the original Meta paper where predictions were independent.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4487089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental Variations and Comparisons\n",
    "\n",
    "# Comparison: Independent vs Causal MTP\n",
    "class IndependentMTP(nn.Module):\n",
    "    \"\"\"\n",
    "    Independent Multi-Token Prediction (like original Meta paper)\n",
    "    Each head predicts independently without hidden state passing\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, vocab_size, num_heads, nhead=8, dim_feedforward=2048):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        self.rms_norm = RMSNorm(d_model)\n",
    "        self.projections = nn.Linear(2 * d_model, d_model)\n",
    "        self.transformer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward,\n",
    "            batch_first=True, dropout=0.1\n",
    "        )\n",
    "        self.unembedding = nn.Linear(d_model, vocab_size)\n",
    "        self.embeddings = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "    def forward(self, input_tokens):\n",
    "        batch_size, seq_len = input_tokens.shape\n",
    "        embeds = self.embeddings(input_tokens)\n",
    "        max_i = seq_len - self.num_heads\n",
    "        all_predictions = []\n",
    "        \n",
    "        for i in range(max_i):\n",
    "            # Use same initial hidden state for ALL heads (no causality)\n",
    "            h_initial = embeds[:, i:i+1, :]\n",
    "            position_predictions = []\n",
    "            \n",
    "            for k in range(self.num_heads):\n",
    "                future_pos = i + k + 1\n",
    "                token_embedding = embeds[:, future_pos:future_pos+1, :]\n",
    "                \n",
    "                # Always use initial hidden state (no causal chain)\n",
    "                h_norm = self.rms_norm(h_initial)\n",
    "                e_norm = self.rms_norm(token_embedding)\n",
    "                merged = torch.cat([h_norm, e_norm], dim=-1)\n",
    "                projected = self.projections(merged)\n",
    "                h_current = self.transformer(projected)\n",
    "                logits_k = self.unembedding(h_current)\n",
    "                \n",
    "                position_predictions.append(logits_k)\n",
    "            \n",
    "            position_logits = torch.cat(position_predictions, dim=1)\n",
    "            all_predictions.append(position_logits)\n",
    "        \n",
    "        return torch.stack(all_predictions, dim=1)\n",
    "\n",
    "# Compare the two approaches\n",
    "print(\"=== Comparing Causal vs Independent MTP ===\")\n",
    "\n",
    "# Create both models\n",
    "causal_model = SimpleMTP(d_model, vocab_size, num_heads, nhead=4, dim_feedforward=32).to(device)\n",
    "independent_model = IndependentMTP(d_model, vocab_size, num_heads, nhead=4, dim_feedforward=32).to(device)\n",
    "\n",
    "# Forward pass with same input\n",
    "with torch.no_grad():\n",
    "    causal_logits = causal_model(input_tokens)\n",
    "    independent_logits = independent_model(input_tokens)\n",
    "\n",
    "# Compare predictions\n",
    "print(f\"Input tokens: {input_tokens[0].tolist()}\")\n",
    "print(f\"\\nPredictions for position i=0:\")\n",
    "\n",
    "causal_preds = torch.argmax(causal_logits[0, 0, :, :], dim=-1)\n",
    "independent_preds = torch.argmax(independent_logits[0, 0, :, :], dim=-1)\n",
    "\n",
    "print(f\"Causal MTP:      {causal_preds.tolist()}\")\n",
    "print(f\"Independent MTP: {independent_preds.tolist()}\")\n",
    "print(f\"Difference:      {(causal_preds != independent_preds).sum().item()} out of {num_heads} predictions differ\")\n",
    "\n",
    "# Compute losses\n",
    "causal_loss = compute_mtp_loss(causal_logits, input_tokens)\n",
    "independent_loss = compute_mtp_loss(independent_logits, input_tokens)\n",
    "\n",
    "print(f\"\\nLoss comparison:\")\n",
    "print(f\"Causal MTP loss:      {causal_loss.item():.6f}\")\n",
    "print(f\"Independent MTP loss: {independent_loss.item():.6f}\")\n",
    "print(f\"Difference:           {causal_loss.item() - independent_loss.item():.6f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Comparison completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ff8e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Simulation and Key Insights\n",
    "\n",
    "def simulate_training_step(model, input_tokens, num_steps=5):\n",
    "    \"\"\"\n",
    "    Simulate a few training steps to show loss improvement\n",
    "    \"\"\"\n",
    "    print(\"=== Training Simulation ===\")\n",
    "    \n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    initial_logits = None\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        # Forward pass\n",
    "        logits = model(input_tokens)\n",
    "        loss = compute_mtp_loss(logits, input_tokens)\n",
    "        \n",
    "        if step == 0:\n",
    "            initial_logits = logits.clone()\n",
    "        \n",
    "        print(f\"Step {step + 1}: Loss = {loss.item():.6f}\")\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Final evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        final_logits = model(input_tokens)\n",
    "        final_loss = compute_mtp_loss(final_logits, input_tokens)\n",
    "    \n",
    "    print(f\"Final loss: {final_loss.item():.6f}\")\n",
    "    \n",
    "    # Compare predictions before and after training\n",
    "    print(f\"\\n=== Prediction Changes ===\")\n",
    "    initial_preds = torch.argmax(initial_logits[0, 0, :, :], dim=-1)\n",
    "    final_preds = torch.argmax(final_logits[0, 0, :, :], dim=-1)\n",
    "    \n",
    "    targets = [input_tokens[0, k + 1].item() for k in range(num_heads)]\n",
    "    \n",
    "    print(f\"Targets:           {targets}\")\n",
    "    print(f\"Initial predictions: {initial_preds.tolist()}\")\n",
    "    print(f\"Final predictions:   {final_preds.tolist()}\")\n",
    "    \n",
    "    # Check accuracy\n",
    "    initial_accuracy = (initial_preds == torch.tensor(targets, device=device)).float().mean().item()\n",
    "    final_accuracy = (final_preds == torch.tensor(targets, device=device)).float().mean().item()\n",
    "    \n",
    "    print(f\"Initial accuracy: {initial_accuracy:.2%}\")\n",
    "    print(f\"Final accuracy:   {final_accuracy:.2%}\")\n",
    "    print(f\"Improvement:      {final_accuracy - initial_accuracy:.2%}\")\n",
    "\n",
    "# Run training simulation\n",
    "simulate_training_step(causal_model, input_tokens)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üéØ KEY INSIGHTS FROM IMPLEMENTATION\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "insights = [\n",
    "    \"1. **Causal Dependencies**: Each prediction head uses hidden state from previous head\",\n",
    "    \"2. **RMS Normalization**: Applied before merging hidden state and input embedding\", \n",
    "    \"3. **Shared Unembedding**: Same vocabulary projection used across all heads\",\n",
    "    \"4. **Sequence Boundaries**: Can only predict from positions with sufficient future context\",\n",
    "    \"5. **Loss Aggregation**: Sum losses across all positions and depths, then average\",\n",
    "    \"6. **Training Benefits**: Richer gradients from multiple prediction targets per token\",\n",
    "    \"7. **Inference Strategy**: DeepSeek discards MTP modules during inference for simplicity\"\n",
    "]\n",
    "\n",
    "for insight in insights:\n",
    "    print(insight)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"üöÄ EXTENSIONS TO EXPERIMENT WITH\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "extensions = [\n",
    "    \"‚Ä¢ **Variable Depth**: Different prediction depths for different positions\",\n",
    "    \"‚Ä¢ **Attention in Heads**: Add attention mechanisms within each prediction head\",\n",
    "    \"‚Ä¢ **Learnable Weights**: Weighted combination of predictions from different depths\",\n",
    "    \"‚Ä¢ **Hierarchical MTP**: Multi-scale token prediction (characters, sub-words, words)\",\n",
    "    \"‚Ä¢ **Conditional MTP**: Prediction depth based on input token uncertainty\",\n",
    "    \"‚Ä¢ **Efficient Implementation**: Parallel processing of independent predictions\"\n",
    "]\n",
    "\n",
    "for extension in extensions:\n",
    "    print(extension)\n",
    "\n",
    "print(f\"\\n‚úÖ Multi-Token Prediction implementation completed successfully!\")\n",
    "print(\"This implementation captures the core concepts from DeepSeek's MTP architecture.\")\n",
    "print(\"You can now experiment with different configurations and extend the functionality!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
