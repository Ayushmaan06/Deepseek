{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a62f375",
   "metadata": {},
   "source": [
    "# Build DeepSeek from Scratch - Multi-Head Attention\n",
    "\n",
    "## Overview\n",
    "This notebook covers the transition from self-attention to multi-head attention, explaining the fundamental motivation, mathematical implementation, and practical benefits of having multiple attention heads in transformer architectures.\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand the limitations of single-head self-attention\n",
    "- Learn why multi-head attention is necessary\n",
    "- Master the step-by-step implementation of multi-head attention\n",
    "- Visualize how different heads capture different perspectives\n",
    "- Prepare for advanced concepts like Key-Value caching and Multi-Head Latent Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6345ad3",
   "metadata": {},
   "source": [
    "## Recap: Self-Attention Foundation\n",
    "\n",
    "### The Four-Step Process\n",
    "**Self-Attention Mechanism**:\n",
    "1. **Transformation**: Input embeddings → Query, Key, Value matrices via trainable weights (WQ, WK, WV)\n",
    "2. **Attention Scores**: Query @ Key.transpose()\n",
    "3. **Attention Weights**: Scale by √d_k, apply softmax, apply causal masking\n",
    "4. **Context Vectors**: Attention weights @ Value vectors\n",
    "\n",
    "### The Achievement\n",
    "- **Input embeddings**: Contain only token semantics + position\n",
    "- **Context vectors**: Contain semantics + relationships with neighboring tokens\n",
    "- **Result**: Richer representations that understand context and relationships\n",
    "\n",
    "### Causal Attention Review\n",
    "- **Constraint**: Tokens can only attend to past and current positions\n",
    "- **Implementation**: Mask upper triangular elements to zero/negative infinity\n",
    "- **Purpose**: Prevents \"cheating\" by looking into the future during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1b081b",
   "metadata": {},
   "source": [
    "## The Problem with Single-Head Attention\n",
    "\n",
    "### Core Limitation: Single Perspective\n",
    "**The fundamental issue**: Self-attention can only capture **one perspective** of a given input sequence.\n",
    "\n",
    "### Ambiguous Sentence Example\n",
    "**Sentence**: \"The artist painted the portrait of a woman with a brush\"\n",
    "\n",
    "**Two Possible Interpretations**:\n",
    "1. **Artist has brush**: The artist used a brush to paint a portrait of a woman\n",
    "2. **Woman has brush**: The artist painted a portrait of a woman who is holding a brush\n",
    "\n",
    "### Attention Matrix Differences\n",
    "\n",
    "**Interpretation 1 - Artist has brush**:\n",
    "```\n",
    "High attention scores:\n",
    "- woman → portrait (she's in the portrait)\n",
    "- artist → brush (artist holds the brush)\n",
    "- brush → artist (brush is with artist)\n",
    "```\n",
    "\n",
    "**Interpretation 2 - Woman has brush**:\n",
    "```\n",
    "High attention scores:\n",
    "- woman → portrait (she's in the portrait)\n",
    "- woman → brush (woman holds the brush)\n",
    "- brush → woman (brush is with woman)\n",
    "- brush → portrait (brush appears in portrait)\n",
    "```\n",
    "\n",
    "### The Single-Head Limitation\n",
    "- **Problem**: Self-attention produces only ONE attention matrix\n",
    "- **Result**: Can only capture ONE interpretation at a time\n",
    "- **Consequence**: Loss of semantic richness and multiple perspectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e2b897",
   "metadata": {},
   "source": [
    "## Why Multi-Head Attention is Necessary\n",
    "\n",
    "### Real-World Text Complexity\n",
    "**Example**: \"The government should regulate free speech\"\n",
    "\n",
    "**Multiple Perspectives**:\n",
    "1. **Restrictive interpretation**: Government should impose restrictions on free speech\n",
    "2. **Protective interpretation**: Government should protect and preserve free speech\n",
    "\n",
    "### The Business Case\n",
    "**Without multi-head attention**:\n",
    "- Model captures only one perspective\n",
    "- Summarization loses nuance\n",
    "- Understanding is impoverished\n",
    "\n",
    "**With multi-head attention**:\n",
    "- Model captures multiple perspectives simultaneously\n",
    "- Richer understanding of ambiguous text\n",
    "- Better summarization and generation\n",
    "\n",
    "### Core Insight\n",
    "**If one self-attention head captures one perspective, then multiple heads can capture multiple perspectives simultaneously.**\n",
    "\n",
    "### The Solution Strategy\n",
    "1. **Multiple self-attention mechanisms** in parallel\n",
    "2. **Each head** captures a different perspective\n",
    "3. **Merge results** to create richer context vectors\n",
    "4. **Same output dimensions** but multiple perspectives embedded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c4a94e",
   "metadata": {},
   "source": [
    "## Multi-Head Attention: Step-by-Step Implementation\n",
    "\n",
    "### Key Parameters\n",
    "- **Input sentence**: \"The artist painted the portrait of a woman with a brush\"\n",
    "- **Number of tokens**: 11\n",
    "- **Input dimension (d_in)**: 8\n",
    "- **Output dimension (d_out)**: 4\n",
    "- **Number of heads**: 2\n",
    "- **Head dimension**: d_out / num_heads = 4 / 2 = 2\n",
    "\n",
    "### Step 1: Start with Input Embeddings\n",
    "```python\n",
    "# Input embedding matrix\n",
    "X = [11 × 8]  # 11 tokens, 8-dimensional embeddings\n",
    "```\n",
    "\n",
    "### Step 2: Single-Head Reference\n",
    "**What single-head attention would do**:\n",
    "```python\n",
    "# Single head matrices\n",
    "WQ = [8 × 4]  # Query weight matrix\n",
    "WK = [8 × 4]  # Key weight matrix  \n",
    "WV = [8 × 4]  # Value weight matrix\n",
    "\n",
    "# Results\n",
    "Q = X @ WQ = [11 × 4]\n",
    "K = X @ WK = [11 × 4]\n",
    "V = X @ WV = [11 × 4]\n",
    "```\n",
    "\n",
    "### Step 3: Split Weight Matrices for Multiple Heads\n",
    "**Key insight**: Split the output dimension among heads\n",
    "\n",
    "```python\n",
    "# Head 1 matrices\n",
    "WQ1 = [8 × 2]  # First 2 columns of original WQ\n",
    "WK1 = [8 × 2]  # First 2 columns of original WK\n",
    "WV1 = [8 × 2]  # First 2 columns of original WV\n",
    "\n",
    "# Head 2 matrices\n",
    "WQ2 = [8 × 2]  # Last 2 columns of original WQ\n",
    "WK2 = [8 × 2]  # Last 2 columns of original WK\n",
    "WV2 = [8 × 2]  # Last 2 columns of original WV\n",
    "```\n",
    "\n",
    "**Critical point**: We're not adding parameters, we're splitting existing ones!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d7df07",
   "metadata": {},
   "source": [
    "### Step 4: Generate Multiple Q, K, V Matrices\n",
    "```python\n",
    "# Head 1 vectors\n",
    "Q1 = X @ WQ1 = [11 × 2]\n",
    "K1 = X @ WK1 = [11 × 2]\n",
    "V1 = X @ WV1 = [11 × 2]\n",
    "\n",
    "# Head 2 vectors\n",
    "Q2 = X @ WQ2 = [11 × 2]\n",
    "K2 = X @ WK2 = [11 × 2]\n",
    "V2 = X @ WV2 = [11 × 2]\n",
    "```\n",
    "\n",
    "**Important observation**: \n",
    "- Number of rows (tokens) remains the same: 11\n",
    "- Number of columns (head dimension) is reduced: 2 instead of 4\n",
    "- We have 2 sets of Q, K, V matrices instead of 1\n",
    "\n",
    "### Step 5: Compute Attention Scores for Each Head\n",
    "```python\n",
    "# Head 1 attention scores\n",
    "Attention_scores_1 = Q1 @ K1.T = [11 × 2] @ [2 × 11] = [11 × 11]\n",
    "\n",
    "# Head 2 attention scores  \n",
    "Attention_scores_2 = Q2 @ K2.T = [11 × 2] @ [2 × 11] = [11 × 11]\n",
    "```\n",
    "\n",
    "**Key insight**: Even though head dimension is reduced (2 vs 4), attention scores matrix remains [11 × 11] because it represents token-to-token relationships.\n",
    "\n",
    "### Step 6: Apply Scaling, Softmax, and Causal Masking\n",
    "```python\n",
    "# For each head, apply the same process:\n",
    "# 1. Scale by √(head_dimension) = √2\n",
    "# 2. Apply softmax\n",
    "# 3. Apply causal masking (upper triangular → 0)\n",
    "# 4. Apply dropout (optional)\n",
    "\n",
    "Attention_weights_1 = process_attention(Attention_scores_1)  # [11 × 11]\n",
    "Attention_weights_2 = process_attention(Attention_scores_2)  # [11 × 11]\n",
    "```\n",
    "\n",
    "### Step 7: Compute Context Vectors for Each Head\n",
    "```python\n",
    "# Head 1 context vectors\n",
    "Context_1 = Attention_weights_1 @ V1 = [11 × 11] @ [11 × 2] = [11 × 2]\n",
    "\n",
    "# Head 2 context vectors\n",
    "Context_2 = Attention_weights_2 @ V2 = [11 × 11] @ [11 × 2] = [11 × 2]\n",
    "```\n",
    "\n",
    "### Step 8: Concatenate Results\n",
    "```python\n",
    "# Merge both heads\n",
    "Final_Context = concat([Context_1, Context_2], dim=-1) = [11 × 4]\n",
    "```\n",
    "\n",
    "**Final result**: Same dimensions as single-head attention [11 × 4], but now contains multiple perspectives!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7c5f4d",
   "metadata": {},
   "source": [
    "## Mathematical Summary\n",
    "\n",
    "### Dimension Tracking\n",
    "```python\n",
    "# Input\n",
    "X: [batch_size, seq_len, d_in] = [1, 11, 8]\n",
    "\n",
    "# Weight matrices (split across heads)\n",
    "WQ_h, WK_h, WV_h: [d_in, head_dim] = [8, 2] for each head\n",
    "\n",
    "# Per-head computations\n",
    "Q_h, K_h, V_h: [batch_size, seq_len, head_dim] = [1, 11, 2]\n",
    "Attention_scores_h: [batch_size, seq_len, seq_len] = [1, 11, 11]\n",
    "Attention_weights_h: [batch_size, seq_len, seq_len] = [1, 11, 11]\n",
    "Context_h: [batch_size, seq_len, head_dim] = [1, 11, 2]\n",
    "\n",
    "# Final concatenation\n",
    "Final_Context: [batch_size, seq_len, d_out] = [1, 11, 4]\n",
    "```\n",
    "\n",
    "### Key Formula\n",
    "```\n",
    "head_dim = d_out / num_heads\n",
    "```\n",
    "\n",
    "### Multi-Head Attention in Matrix Form\n",
    "```python\n",
    "def multi_head_attention(X, num_heads):\n",
    "    head_dim = d_out // num_heads\n",
    "    \n",
    "    # Split weight matrices\n",
    "    WQ_heads = split(WQ, num_heads, dim=-1)\n",
    "    WK_heads = split(WK, num_heads, dim=-1)  \n",
    "    WV_heads = split(WV, num_heads, dim=-1)\n",
    "    \n",
    "    contexts = []\n",
    "    for i in range(num_heads):\n",
    "        Q_i = X @ WQ_heads[i]\n",
    "        K_i = X @ WK_heads[i]\n",
    "        V_i = X @ WV_heads[i]\n",
    "        \n",
    "        scores_i = Q_i @ K_i.T / sqrt(head_dim)\n",
    "        weights_i = softmax(causal_mask(scores_i))\n",
    "        context_i = weights_i @ V_i\n",
    "        \n",
    "        contexts.append(context_i)\n",
    "    \n",
    "    return concat(contexts, dim=-1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bb5e3c",
   "metadata": {},
   "source": [
    "## Advantages and Trade-offs\n",
    "\n",
    "### Advantages of Multi-Head Attention\n",
    "1. **Multiple Perspectives**: Each head can capture different semantic relationships\n",
    "2. **Richer Representations**: Final context vectors contain diverse information\n",
    "3. **Specialized Learning**: Heads can specialize in different aspects:\n",
    "   - Head 1: Syntactic relationships (subject-verb-object)\n",
    "   - Head 2: Semantic relationships (word meanings)\n",
    "   - Head 3: Positional relationships (temporal sequences)\n",
    "4. **Same Output Dimensions**: No increase in final output size\n",
    "5. **Parallel Processing**: All heads computed simultaneously\n",
    "\n",
    "### Trade-offs\n",
    "1. **Reduced Per-Head Capacity**: Each head has fewer dimensions to work with\n",
    "   - Single head: 4 dimensions per head\n",
    "   - Multi-head (2 heads): 2 dimensions per head\n",
    "2. **Divide and Conquer**: Trade individual head expressivity for multiple perspectives\n",
    "3. **Computational Overhead**: More matrix operations (but parallelizable)\n",
    "\n",
    "### The Trade-off Analysis\n",
    "**Single Head**: \n",
    "- ✅ Full dimensional capacity (4D)\n",
    "- ❌ Single perspective only\n",
    "\n",
    "**Multi-Head**: \n",
    "- ✅ Multiple perspectives (2 heads)\n",
    "- ❌ Reduced dimensional capacity per head (2D each)\n",
    "- ✅ Net gain in representational power\n",
    "\n",
    "**Empirical Evidence**: Multi-head attention consistently outperforms single-head attention across all major language tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c69f12",
   "metadata": {},
   "source": [
    "## Practical Demonstration: Visualizing Different Heads\n",
    "\n",
    "### Setup\n",
    "- **Model**: Pre-trained BERT (bidirectional attention)\n",
    "- **Sentence**: \"The artist painted the portrait of a woman with a brush\"\n",
    "- **Analysis**: Layer 3, Heads 3 and 8 (out of 12 total heads)\n",
    "- **Focus Token**: \"woman\"\n",
    "\n",
    "### Head 3 Analysis\n",
    "**Query Token**: \"woman\"\n",
    "**Highest Attention**: \"brush\"\n",
    "\n",
    "**Interpretation**: This head seems to capture the perspective where the woman is holding the brush.\n",
    "\n",
    "### Head 8 Analysis  \n",
    "**Query Token**: \"woman\"\n",
    "**Highest Attention**: \"portrait\"\n",
    "\n",
    "**Interpretation**: This head captures the perspective where the woman is the subject of the portrait (not necessarily holding the brush).\n",
    "\n",
    "### Visualization Code Structure\n",
    "```python\n",
    "# Load pre-trained model\n",
    "model = load_pretrained_bert()\n",
    "\n",
    "# Extract attention weights\n",
    "attention_weights = model.get_attention_weights(\n",
    "    sentence=\"The artist painted the portrait of a woman with a brush\",\n",
    "    layer=3,\n",
    "    head=[3, 8]\n",
    ")\n",
    "\n",
    "# Visualize attention patterns\n",
    "visualize_attention(attention_weights, focus_token=\"woman\")\n",
    "```\n",
    "\n",
    "### Key Insights from Demo\n",
    "1. **Head 3**: woman → brush (high attention)\n",
    "   - Suggests: \"woman with a brush\" interpretation\n",
    "2. **Head 8**: woman → portrait (high attention)  \n",
    "   - Suggests: \"portrait of a woman\" interpretation\n",
    "3. **Different heads capture different semantic relationships**\n",
    "4. **Pre-trained models naturally learn diverse perspectives**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc6300f",
   "metadata": {},
   "source": [
    "## Implementation Considerations\n",
    "\n",
    "### Parameter Count Analysis\n",
    "**Single Head**:\n",
    "```python\n",
    "WQ: [d_in × d_out] = [8 × 4] = 32 parameters\n",
    "WK: [d_in × d_out] = [8 × 4] = 32 parameters  \n",
    "WV: [d_in × d_out] = [8 × 4] = 32 parameters\n",
    "Total: 96 parameters\n",
    "```\n",
    "\n",
    "**Multi-Head (2 heads)**:\n",
    "```python\n",
    "WQ1: [8 × 2] = 16 parameters\n",
    "WK1: [8 × 2] = 16 parameters\n",
    "WV1: [8 × 2] = 16 parameters\n",
    "WQ2: [8 × 2] = 16 parameters\n",
    "WK2: [8 × 2] = 16 parameters\n",
    "WV2: [8 × 2] = 16 parameters\n",
    "Total: 96 parameters\n",
    "```\n",
    "\n",
    "**Key insight**: Same number of parameters, just organized differently!\n",
    "\n",
    "### Memory Requirements\n",
    "**Attention Matrices**: Each head requires [seq_len × seq_len] attention matrix\n",
    "- Single head: 1 × [11 × 11] = 121 elements\n",
    "- Multi-head: 2 × [11 × 11] = 242 elements\n",
    "- **Memory scales linearly with number of heads**\n",
    "\n",
    "### Computational Complexity\n",
    "**Per head**: O(seq_len² × head_dim)\n",
    "**Total**: O(seq_len² × d_out) - same as single head!\n",
    "\n",
    "The computation per head is smaller but we have more heads, so total computation remains the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee71188",
   "metadata": {},
   "source": [
    "## Connection to Modern LLMs\n",
    "\n",
    "### Why All Modern LLMs Use Multi-Head Attention\n",
    "1. **GPT models**: 12-96 attention heads per layer\n",
    "2. **BERT models**: 12-16 attention heads per layer  \n",
    "3. **T5 models**: 12-32 attention heads per layer\n",
    "4. **DeepSeek**: Advanced multi-head latent attention\n",
    "\n",
    "### Head Specialization in Practice\n",
    "**Research findings show heads often specialize in**:\n",
    "- **Syntactic heads**: Subject-verb relationships, dependency parsing\n",
    "- **Semantic heads**: Word meaning relationships, entity recognition\n",
    "- **Positional heads**: Sequential patterns, temporal relationships\n",
    "- **Attention heads**: Long-range dependencies, discourse structure\n",
    "\n",
    "### The Path Forward\n",
    "**Next steps in our journey**:\n",
    "1. ✅ Self-attention\n",
    "2. ✅ Causal attention  \n",
    "3. ✅ Multi-head attention\n",
    "4. 🔄 **Next**: Key-Value caching (efficiency optimization)\n",
    "5. 🔄 **Final**: Multi-head latent attention (DeepSeek's innovation)\n",
    "\n",
    "### Why This Foundation Matters\n",
    "- **KV caching**: Optimizes the key-value computations we just learned\n",
    "- **Multi-head latent attention**: Modifies the multi-head structure for better efficiency\n",
    "- **Understanding prerequisites**: Can't understand advanced concepts without mastering these basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858c50a1",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Core Concepts\n",
    "1. **Problem**: Single-head attention captures only one perspective\n",
    "2. **Solution**: Multiple heads capture multiple perspectives simultaneously  \n",
    "3. **Implementation**: Split weight matrices across heads, compute in parallel, concatenate results\n",
    "4. **Trade-off**: Reduced per-head capacity for increased perspective diversity\n",
    "\n",
    "### Mathematical Insights\n",
    "- **Same parameter count**: Multi-head doesn't add parameters, just reorganizes them\n",
    "- **Same output dimensions**: Final context vectors have same size as single-head\n",
    "- **Same computational complexity**: O(seq_len² × d_out) regardless of head count\n",
    "- **Linear memory scaling**: Memory increases with number of heads\n",
    "\n",
    "### Practical Benefits\n",
    "- **Richer representations**: Multiple perspectives embedded in same output\n",
    "- **Specialized learning**: Heads can focus on different aspects of language\n",
    "- **Better performance**: Empirically superior across all language tasks\n",
    "- **Interpretability**: Can visualize and understand what each head learns\n",
    "\n",
    "### The Foundation for Advanced Concepts\n",
    "Multi-head attention is the essential building block for:\n",
    "- **Transformer architecture**: Core component of all modern LLMs\n",
    "- **Efficiency optimizations**: KV caching, gradient checkpointing\n",
    "- **Architectural innovations**: Multi-head latent attention, mixture of experts\n",
    "- **Understanding language**: How models capture complex linguistic relationships\n",
    "\n",
    "This mechanism is fundamental to how language models understand and generate human language, making it one of the most important concepts in modern AI."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
