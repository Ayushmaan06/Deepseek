{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35d8d0aa",
   "metadata": {},
   "source": [
    "# Build DeepSeek from Scratch - Multi-Head Attention Implementation\n",
    "\n",
    "## Overview\n",
    "This notebook provides a comprehensive mathematical walkthrough and Python implementation of multi-head attention. We'll see every matrix operation step-by-step, understand the dimensional transformations, and implement the complete multi-head attention class.\n",
    "\n",
    "## Learning Objectives\n",
    "- Master the mathematical implementation of multi-head attention\n",
    "- Understand every matrix multiplication and dimensional transformation\n",
    "- Learn how to reshape and group matrices for multiple heads\n",
    "- Implement the complete multi-head attention class in Python\n",
    "- Bridge the gap between conceptual understanding and practical coding\n",
    "\n",
    "## Prerequisites\n",
    "- Understanding of self-attention mechanism\n",
    "- Knowledge of causal attention and masking\n",
    "- Familiarity with the conceptual overview of multi-head attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9050d4b",
   "metadata": {},
   "source": [
    "## Recap: Multi-Head Attention Concept\n",
    "\n",
    "### The Core Problem\n",
    "**Single-head attention limitation**: Can only capture one perspective of input text\n",
    "\n",
    "**Example**: \"The artist painted the portrait of a woman with a brush\"\n",
    "- **Perspective 1**: Artist uses brush to paint\n",
    "- **Perspective 2**: Woman in portrait holds brush\n",
    "\n",
    "### The Solution Strategy\n",
    "1. **Split** query, key, value matrices into multiple heads\n",
    "2. **Process** each head independently to capture different perspectives  \n",
    "3. **Merge** results to create richer context vectors\n",
    "4. **Maintain** same output dimensions as single-head attention\n",
    "\n",
    "### Key Parameters for Today's Implementation\n",
    "- **Input tokens**: 3 (simplified example)\n",
    "- **Input dimension (d_in)**: 6\n",
    "- **Output dimension (d_out)**: 6\n",
    "- **Number of heads**: 2\n",
    "- **Head dimension**: d_out / num_heads = 6 / 2 = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096e1cdb",
   "metadata": {},
   "source": [
    "## Understanding the Key Parameters with Examples\n",
    "\n",
    "Let's break down each parameter with concrete, intuitive examples:\n",
    "\n",
    "### 1. **Input tokens = 3** (Sequence Length)\n",
    "This means we're processing a sentence with 3 words/tokens.\n",
    "\n",
    "**Example sentence**: \"The cat sleeps\"\n",
    "- Token 1: \"The\" \n",
    "- Token 2: \"cat\"\n",
    "- Token 3: \"sleeps\"\n",
    "\n",
    "**In practice**: Real sentences have hundreds or thousands of tokens, but we use 3 for simplicity.\n",
    "\n",
    "### 2. **Input dimension (d_in) = 6** (Embedding Size)\n",
    "Each token is represented by a 6-dimensional vector (usually 512-4096 in real models).\n",
    "\n",
    "**Example for \"The\"**:\n",
    "```\n",
    "\"The\" → [0.2, -0.1, 0.8, 0.3, -0.5, 0.7]  # 6 numbers\n",
    "```\n",
    "\n",
    "**What these numbers represent**:\n",
    "- Position 1: Maybe \"definiteness\" (0.2 = somewhat definite)\n",
    "- Position 2: Maybe \"animacy\" (-0.1 = not animate)\n",
    "- Position 3: Maybe \"frequency\" (0.8 = very common word)\n",
    "- Position 4: Maybe \"grammatical role\" (0.3 = article)\n",
    "- Position 5: Maybe \"sentiment\" (-0.5 = neutral)\n",
    "- Position 6: Maybe \"semantic category\" (0.7 = function word)\n",
    "\n",
    "### 3. **Output dimension (d_out) = 6** (Context Size)\n",
    "After attention, each token still has 6 dimensions, but now enriched with context.\n",
    "\n",
    "**Before attention** - \"cat\" in isolation:\n",
    "```\n",
    "[0.5, 0.9, 0.1, -0.2, 0.3, 0.8]  # Just \"cat\" features\n",
    "```\n",
    "\n",
    "**After attention** - \"cat\" with context from \"The\" and \"sleeps\":\n",
    "```\n",
    "[0.6, 0.8, 0.2, 0.1, 0.4, 0.7]   # \"cat\" + context from other words\n",
    "```\n",
    "\n",
    "### 4. **Number of heads = 2** (Multiple Perspectives)\n",
    "We split the attention into 2 different \"views\" of the same sentence.\n",
    "\n",
    "**Head 1 might focus on**: Grammar relationships\n",
    "- \"The\" → \"cat\" (determiner-noun relationship)\n",
    "- \"cat\" → \"sleeps\" (subject-verb relationship)\n",
    "\n",
    "**Head 2 might focus on**: Semantic relationships  \n",
    "- \"The\" → \"sleeps\" (who is doing the sleeping?)\n",
    "- \"cat\" → \"sleeps\" (what kind of sleeping? peaceful, deep, etc.)\n",
    "\n",
    "### 5. **Head dimension = 3** (d_out ÷ num_heads)\n",
    "Each head works with 3 dimensions instead of all 6.\n",
    "\n",
    "**Original 6 dimensions** split into **2 heads of 3 dimensions each**:\n",
    "\n",
    "**Head 1 gets dimensions 1-3**:\n",
    "```\n",
    "\"The\": [0.2, -0.1, 0.8] → focuses on grammar\n",
    "\"cat\": [0.5, 0.9, 0.1] → focuses on grammar  \n",
    "\"sleeps\": [0.1, 0.4, 0.6] → focuses on grammar\n",
    "```\n",
    "\n",
    "**Head 2 gets dimensions 4-6**:\n",
    "```\n",
    "\"The\": [0.3, -0.5, 0.7] → focuses on meaning\n",
    "\"cat\": [-0.2, 0.3, 0.8] → focuses on meaning\n",
    "\"sleeps\": [0.2, 0.1, 0.9] → focuses on meaning\n",
    "```\n",
    "\n",
    "### Real-World Analogy\n",
    "Think of it like **two people reading the same sentence**:\n",
    "\n",
    "**Person 1 (Head 1)**: Grammar expert\n",
    "- Notices: \"The\" is an article, \"cat\" is a noun, \"sleeps\" is a verb\n",
    "- Focuses on: Sentence structure, word roles, syntax\n",
    "\n",
    "**Person 2 (Head 2)**: Meaning expert  \n",
    "- Notices: This is about an animal, the action is peaceful, it's present tense\n",
    "- Focuses on: Semantics, concepts, relationships\n",
    "\n",
    "**Final result**: Combine both perspectives for richer understanding!\n",
    "\n",
    "### Why These Numbers?\n",
    "- **Small numbers** (3 tokens, 6 dimensions) make it easy to follow the math\n",
    "- **Real models** use much larger numbers (1000+ tokens, 512+ dimensions)\n",
    "- **Same principles** apply regardless of size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1db73fa",
   "metadata": {},
   "source": [
    "## Understanding Key Parameters with Examples\n",
    "\n",
    "### 1. **Input Tokens = 3** (Sequence Length)\n",
    "\n",
    "**What it means**: We're processing 3 words/tokens at once\n",
    "\n",
    "**Real Example**:\n",
    "```\n",
    "Input sentence: \"The cat sat\"\n",
    "Token 1: \"The\"\n",
    "Token 2: \"cat\"  \n",
    "Token 3: \"sat\"\n",
    "```\n",
    "\n",
    "**Why 3 tokens?**\n",
    "- Small example for easy understanding\n",
    "- In real models: 512, 1024, 2048+ tokens\n",
    "- Each token represents one word/subword\n",
    "\n",
    "### 2. **Input Dimension (d_in) = 6** (Embedding Size)\n",
    "\n",
    "**What it means**: Each token is represented by a 6-dimensional vector\n",
    "\n",
    "**Real Example**:\n",
    "```\n",
    "\"The\" → [0.1, -0.3, 0.7, 0.2, -0.1, 0.5]  # 6 numbers\n",
    "\"cat\" → [0.4, 0.8, -0.2, 0.3, 0.6, -0.4]  # 6 numbers  \n",
    "\"sat\" → [-0.2, 0.1, 0.9, -0.5, 0.3, 0.7]  # 6 numbers\n",
    "```\n",
    "\n",
    "**Why 6 dimensions?**\n",
    "- Small example for easy math\n",
    "- In real models: 768 (BERT), 1024 (GPT-2), 4096+ (GPT-3)\n",
    "- Each dimension captures different semantic features\n",
    "\n",
    "### 3. **Output Dimension (d_out) = 6** (Context Size)\n",
    "\n",
    "**What it means**: Each token produces a 6-dimensional context vector\n",
    "\n",
    "**Real Example**:\n",
    "```\n",
    "Input:  \"The\" → [0.1, -0.3, 0.7, 0.2, -0.1, 0.5]\n",
    "Output: \"The\" → [0.3, 0.1, -0.2, 0.8, 0.4, -0.1]  # New context-aware representation\n",
    "```\n",
    "\n",
    "**Why same as input?**\n",
    "- Common in transformers (residual connections work better)\n",
    "- Could be different (e.g., d_in=512, d_out=1024)\n",
    "- Output contains richer context information\n",
    "\n",
    "### 4. **Number of Heads = 2** (Multiple Perspectives)\n",
    "\n",
    "**What it means**: We split attention into 2 different \"viewpoints\"\n",
    "\n",
    "**Real Example with \"The cat sat\"**:\n",
    "```\n",
    "Head 1 might focus on: Grammar relationships\n",
    "- \"The\" pays attention to \"cat\" (article → noun)\n",
    "- \"cat\" pays attention to \"sat\" (subject → verb)\n",
    "\n",
    "Head 2 might focus on: Semantic relationships  \n",
    "- \"cat\" pays attention to \"sat\" (who does what)\n",
    "- \"sat\" pays attention to \"The cat\" (complete subject)\n",
    "```\n",
    "\n",
    "**Why 2 heads?**\n",
    "- Simple example for learning\n",
    "- Real models: 8, 12, 16+ heads\n",
    "- Each head learns different patterns\n",
    "\n",
    "### 5. **Head Dimension = 3** (Per-Head Size)\n",
    "\n",
    "**What it means**: Each head processes 3-dimensional vectors\n",
    "\n",
    "**Mathematical relationship**:\n",
    "```\n",
    "head_dim = d_out / num_heads = 6 / 2 = 3\n",
    "```\n",
    "\n",
    "**Real Example**:\n",
    "```\n",
    "Original \"The\" vector: [0.1, -0.3, 0.7, 0.2, -0.1, 0.5]\n",
    "\n",
    "Split into 2 heads:\n",
    "Head 1: [0.1, -0.3, 0.7]  # First 3 dimensions\n",
    "Head 2: [0.2, -0.1, 0.5]  # Last 3 dimensions\n",
    "```\n",
    "\n",
    "**Why this split?**\n",
    "- Allows parallel processing\n",
    "- Each head has smaller, focused representation\n",
    "- Final output recombines all heads\n",
    "\n",
    "### Visual Summary\n",
    "\n",
    "```\n",
    "Input Matrix (3 tokens × 6 dimensions):\n",
    "[0.1, -0.3, 0.7, 0.2, -0.1, 0.5]  ← \"The\"\n",
    "[0.4,  0.8,-0.2, 0.3,  0.6,-0.4]  ← \"cat\"\n",
    "[-0.2, 0.1, 0.9,-0.5,  0.3, 0.7]  ← \"sat\"\n",
    "\n",
    "↓ Multi-Head Attention Processing ↓\n",
    "\n",
    "Output Matrix (3 tokens × 6 dimensions):\n",
    "[0.3,  0.1,-0.2, 0.8,  0.4,-0.1]  ← \"The\" (context-aware)\n",
    "[0.5, -0.3, 0.6, 0.2, -0.7, 0.4]  ← \"cat\" (context-aware)\n",
    "[0.1,  0.9,-0.1, 0.3,  0.5,-0.2]  ← \"sat\" (context-aware)\n",
    "```\n",
    "\n",
    "**Key Insight**: Same input/output dimensions, but output vectors now contain information from ALL tokens in the sequence, processed through multiple attention heads!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a700c02",
   "metadata": {},
   "source": [
    "## Step 1: Input Setup and Initialization\n",
    "\n",
    "### Input Embedding Matrix Structure\n",
    "```python\n",
    "# Input shape: [batch_size, num_tokens, d_in]\n",
    "X = [1, 3, 6]  # 1 batch, 3 tokens, 6-dimensional embeddings\n",
    "```\n",
    "\n",
    "### Visual Representation\n",
    "```\n",
    "Token 1: [x₁₁, x₁₂, x₁₃, x₁₄, x₁₅, x₁₆]  # Input embedding\n",
    "Token 2: [x₂₁, x₂₂, x₂₃, x₂₄, x₂₅, x₂₆]  # Input embedding  \n",
    "Token 3: [x₃₁, x₃₂, x₃₃, x₃₄, x₃₅, x₃₆]  # Input embedding\n",
    "```\n",
    "\n",
    "### Key Dimensions to Track\n",
    "- **Batch size**: 1 (can be extended to multiple batches)\n",
    "- **Number of tokens**: 3 (sequence length)\n",
    "- **Input dimension**: 6 (d_in)\n",
    "- **Output dimension**: 6 (d_out)\n",
    "\n",
    "### Input Embedding Composition\n",
    "**Recall**: Input embedding = Token embedding + Positional embedding\n",
    "- Each token gets its own \"uniform\" representation\n",
    "- Contains both semantic and positional information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fcdcbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input X shape: torch.Size([1, 3, 6])\n",
      "X:\n",
      "tensor([[[ 1.0155, -0.5620,  0.1173,  0.5436, -0.4497, -0.4543],\n",
      "         [-0.3182,  0.6049, -1.0629, -0.9308, -1.8998,  1.4582],\n",
      "         [-0.6771, -1.4818,  0.1613,  1.3851,  2.0653,  0.8286]]])\n",
      "\n",
      "Key parameters:\n",
      "- Input dimension (d_in): 6\n",
      "- Output dimension (d_out): 6\n",
      "- Number of heads: 2\n",
      "- Head dimension: 3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# Set up example input\n",
    "batch_size = 1\n",
    "num_tokens = 3\n",
    "d_in = 6\n",
    "d_out = 6\n",
    "num_heads = 2\n",
    "head_dim = d_out // num_heads  # = 3\n",
    "\n",
    "# Create example input embedding matrix\n",
    "# Shape: [batch_size, num_tokens, d_in]\n",
    "X = torch.randn(batch_size, num_tokens, d_in)\n",
    "print(f\"Input X shape: {X.shape}\")\n",
    "print(f\"X:\\n{X}\")\n",
    "\n",
    "print(f\"\\nKey parameters:\")\n",
    "print(f\"- Input dimension (d_in): {d_in}\")\n",
    "print(f\"- Output dimension (d_out): {d_out}\")\n",
    "print(f\"- Number of heads: {num_heads}\")\n",
    "print(f\"- Head dimension: {head_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24ab265",
   "metadata": {},
   "source": [
    "## Step 2: Initialize Trainable Weight Matrices\n",
    "\n",
    "### Weight Matrix Dimensions\n",
    "All weight matrices have shape `[d_in, d_out]` = `[6, 6]`\n",
    "\n",
    "```python\n",
    "WQ: [6 × 6]  # Query weight matrix\n",
    "WK: [6 × 6]  # Key weight matrix  \n",
    "WV: [6 × 6]  # Value weight matrix\n",
    "```\n",
    "\n",
    "### Key Insight\n",
    "- **Same parameter count** as single-head attention\n",
    "- **Different organization**: Will be split across heads later\n",
    "- **Random initialization**: Optimized through backpropagation\n",
    "\n",
    "### Matrix Multiplication Overview\n",
    "```python\n",
    "# Standard linear transformations\n",
    "Q = X @ WQ  # [1, 3, 6] @ [6, 6] = [1, 3, 6]\n",
    "K = X @ WK  # [1, 3, 6] @ [6, 6] = [1, 3, 6]  \n",
    "V = X @ WV  # [1, 3, 6] @ [6, 6] = [1, 3, 6]\n",
    "```\n",
    "\n",
    "### Dimensional Analysis\n",
    "- **Input**: [batch_size, num_tokens, d_in]\n",
    "- **Output**: [batch_size, num_tokens, d_out]\n",
    "- **Space transformation**: From input dimension space to output dimension space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca03adf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q shape: torch.Size([1, 3, 6])\n",
      "K shape: torch.Size([1, 3, 6])\n",
      "V shape: torch.Size([1, 3, 6])\n",
      "\n",
      "Q matrix:\n",
      "tensor([[[ 0.2434,  0.4607, -0.5537, -0.5116, -0.0451,  0.1184],\n",
      "         [-0.5975, -0.5909, -0.6584, -0.2954, -0.6365, -0.7123],\n",
      "         [ 0.4812, -0.1247,  0.3195,  1.0179,  0.8944,  0.8886]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "K matrix:\n",
      "tensor([[[-0.3222,  0.3691,  0.3103, -0.5221, -0.0345,  0.4966],\n",
      "         [-0.5679,  0.7716,  0.3563, -0.4399,  1.3386,  0.2529],\n",
      "         [ 0.5660,  0.5104, -0.6236,  1.3696, -0.8633, -0.0945]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "V matrix:\n",
      "tensor([[[-0.8460,  0.2317,  0.0061, -0.1790,  0.0405,  0.0707],\n",
      "         [ 1.4305, -0.4608,  1.1821,  1.2324,  0.0492, -0.3842],\n",
      "         [-0.3349,  1.5204, -1.7049, -0.3751,  0.8196,  0.6283]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "✓ All Q, K, V matrices have correct dimensions\n"
     ]
    }
   ],
   "source": [
    "# Initialize trainable weight matrices\n",
    "# Using nn.Linear with bias=False for optimized initialization\n",
    "W_query = nn.Linear(d_in, d_out, bias=False)\n",
    "W_key = nn.Linear(d_in, d_out, bias=False)\n",
    "W_value = nn.Linear(d_in, d_out, bias=False)\n",
    "\n",
    "# Generate Q, K, V matrices\n",
    "Q = W_query(X)  # [1, 3, 6]\n",
    "K = W_key(X)    # [1, 3, 6]\n",
    "V = W_value(X)  # [1, 3, 6]\n",
    "\n",
    "print(f\"Q shape: {Q.shape}\")\n",
    "print(f\"K shape: {K.shape}\")\n",
    "print(f\"V shape: {V.shape}\")\n",
    "\n",
    "print(f\"\\nQ matrix:\\n{Q}\")\n",
    "print(f\"\\nK matrix:\\n{K}\")\n",
    "print(f\"\\nV matrix:\\n{V}\")\n",
    "\n",
    "# Verify dimensions match expected output space\n",
    "assert Q.shape == (batch_size, num_tokens, d_out)\n",
    "assert K.shape == (batch_size, num_tokens, d_out)\n",
    "assert V.shape == (batch_size, num_tokens, d_out)\n",
    "print(\"\\n✓ All Q, K, V matrices have correct dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c913d8",
   "metadata": {},
   "source": [
    "## Step 3: Reshape for Multiple Heads (Unrolling)\n",
    "\n",
    "### The Critical Transformation\n",
    "**Goal**: Split the output dimension across multiple heads\n",
    "\n",
    "**Before**: `[batch_size, num_tokens, d_out]` = `[1, 3, 6]`\n",
    "**After**: `[batch_size, num_tokens, num_heads, head_dim]` = `[1, 3, 2, 3]`\n",
    "\n",
    "### Visual Understanding of Unrolling\n",
    "\n",
    "**Original Q matrix** (6 columns):\n",
    "```\n",
    "Token 1: [q₁₁, q₁₂, q₁₃, q₁₄, q₁₅, q₁₆]\n",
    "Token 2: [q₂₁, q₂₂, q₂₃, q₂₄, q₂₅, q₂₆]\n",
    "Token 3: [q₃₁, q₃₂, q₃₃, q₃₄, q₃₅, q₃₆]\n",
    "```\n",
    "\n",
    "**After unrolling** (2 heads × 3 dimensions each):\n",
    "```\n",
    "Token 1, Head 1: [q₁₁, q₁₂, q₁₃]  Token 1, Head 2: [q₁₄, q₁₅, q₁₆]\n",
    "Token 2, Head 1: [q₂₁, q₂₂, q₂₃]  Token 2, Head 2: [q₂₄, q₂₅, q₂₆]\n",
    "Token 3, Head 1: [q₃₁, q₃₂, q₃₃]  Token 3, Head 2: [q₃₄, q₃₅, q₃₆]\n",
    "```\n",
    "\n",
    "### Dimension Formula\n",
    "```\n",
    "head_dim = d_out / num_heads\n",
    "[B, T, D] → [B, T, H, head_dim]\n",
    "```\n",
    "\n",
    "### Why This Works\n",
    "- **No parameter addition**: Just reorganization\n",
    "- **Head isolation**: Each head gets its own subspace\n",
    "- **Parallel processing**: All heads computed simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "826c7e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After unrolling:\n",
      "Q_reshaped shape: torch.Size([1, 3, 2, 3])\n",
      "K_reshaped shape: torch.Size([1, 3, 2, 3])\n",
      "V_reshaped shape: torch.Size([1, 3, 2, 3])\n",
      "\n",
      "Q_reshaped:\n",
      "tensor([[[[ 0.2434,  0.4607, -0.5537],\n",
      "          [-0.5116, -0.0451,  0.1184]],\n",
      "\n",
      "         [[-0.5975, -0.5909, -0.6584],\n",
      "          [-0.2954, -0.6365, -0.7123]],\n",
      "\n",
      "         [[ 0.4812, -0.1247,  0.3195],\n",
      "          [ 1.0179,  0.8944,  0.8886]]]], grad_fn=<ViewBackward0>)\n",
      "\n",
      "Verification - Original Q flattened equals reshaped Q flattened:\n",
      "Original: tensor([ 0.2434,  0.4607, -0.5537, -0.5116, -0.0451,  0.1184],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Reshaped: tensor([ 0.2434,  0.4607, -0.5537, -0.5116, -0.0451,  0.1184],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Equal: True\n",
      "\n",
      "Head separation visualization:\n",
      "Token 1, Head 1: tensor([ 0.2434,  0.4607, -0.5537], grad_fn=<SliceBackward0>)\n",
      "Token 1, Head 2: tensor([-0.5116, -0.0451,  0.1184], grad_fn=<SliceBackward0>)\n",
      "Token 2, Head 1: tensor([-0.5975, -0.5909, -0.6584], grad_fn=<SliceBackward0>)\n",
      "Token 2, Head 2: tensor([-0.2954, -0.6365, -0.7123], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Unroll last dimension to create multiple heads\n",
    "# Transform from [B, T, D] to [B, T, H, head_dim]\n",
    "\n",
    "Q_reshaped = Q.view(batch_size, num_tokens, num_heads, head_dim)\n",
    "K_reshaped = K.view(batch_size, num_tokens, num_heads, head_dim)\n",
    "V_reshaped = V.view(batch_size, num_tokens, num_heads, head_dim)\n",
    "\n",
    "print(f\"After unrolling:\")\n",
    "print(f\"Q_reshaped shape: {Q_reshaped.shape}\")\n",
    "print(f\"K_reshaped shape: {K_reshaped.shape}\")\n",
    "print(f\"V_reshaped shape: {V_reshaped.shape}\")\n",
    "\n",
    "print(f\"\\nQ_reshaped:\\n{Q_reshaped}\")\n",
    "\n",
    "# Verify the reshape preserved the data\n",
    "print(f\"\\nVerification - Original Q flattened equals reshaped Q flattened:\")\n",
    "print(f\"Original: {Q.flatten()[:6]}\")\n",
    "print(f\"Reshaped: {Q_reshaped.flatten()[:6]}\")\n",
    "print(f\"Equal: {torch.allclose(Q.flatten(), Q_reshaped.flatten())}\")\n",
    "\n",
    "# Visualize the head separation\n",
    "print(f\"\\nHead separation visualization:\")\n",
    "print(f\"Token 1, Head 1: {Q_reshaped[0, 0, 0, :]}\")  # [q11, q12, q13]\n",
    "print(f\"Token 1, Head 2: {Q_reshaped[0, 0, 1, :]}\")  # [q14, q15, q16]\n",
    "print(f\"Token 2, Head 1: {Q_reshaped[0, 1, 0, :]}\")  # [q21, q22, q23]\n",
    "print(f\"Token 2, Head 2: {Q_reshaped[0, 1, 1, :]}\")  # [q24, q25, q26]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3646818",
   "metadata": {},
   "source": [
    "## Step 4: Transpose for Head-Grouped Processing\n",
    "\n",
    "### The Grouping Problem\n",
    "**Current grouping**: By tokens `[B, T, H, head_dim]`\n",
    "- Token 1: [Head 1, Head 2]\n",
    "- Token 2: [Head 1, Head 2]  \n",
    "- Token 3: [Head 1, Head 2]\n",
    "\n",
    "**Desired grouping**: By heads `[B, H, T, head_dim]`\n",
    "- Head 1: [Token 1, Token 2, Token 3]\n",
    "- Head 2: [Token 1, Token 2, Token 3]\n",
    "\n",
    "### Why Transpose is Necessary\n",
    "**For efficient computation**:\n",
    "- Q1 @ K1.T (Head 1 processing)\n",
    "- Q2 @ K2.T (Head 2 processing)\n",
    "\n",
    "**Need clear head separation**:\n",
    "- All Q1 data together\n",
    "- All K1 data together\n",
    "- All Q2 data together\n",
    "- All K2 data together\n",
    "\n",
    "### Transpose Operation\n",
    "```python\n",
    "# Swap dimensions 1 and 2\n",
    ".transpose(1, 2)  # [B, T, H, head_dim] → [B, H, T, head_dim]\n",
    "```\n",
    "\n",
    "### Visual Result After Transpose\n",
    "```\n",
    "Head 1:\n",
    "  Token 1: [q₁₁, q₁₂, q₁₃]\n",
    "  Token 2: [q₂₁, q₂₂, q₂₃]  \n",
    "  Token 3: [q₃₁, q₃₂, q₃₃]\n",
    "\n",
    "Head 2:\n",
    "  Token 1: [q₁₄, q₁₅, q₁₆]\n",
    "  Token 2: [q₂₄, q₂₅, q₂₆]\n",
    "  Token 3: [q₃₄, q₃₅, q₃₆]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fb4229b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After transpose (grouped by heads):\n",
      "Q_heads shape: torch.Size([1, 2, 3, 3])\n",
      "K_heads shape: torch.Size([1, 2, 3, 3])\n",
      "V_heads shape: torch.Size([1, 2, 3, 3])\n",
      "\n",
      "Q_heads structure:\n",
      "Q_heads:\n",
      "tensor([[[[ 0.2434,  0.4607, -0.5537],\n",
      "          [-0.5975, -0.5909, -0.6584],\n",
      "          [ 0.4812, -0.1247,  0.3195]],\n",
      "\n",
      "         [[-0.5116, -0.0451,  0.1184],\n",
      "          [-0.2954, -0.6365, -0.7123],\n",
      "          [ 1.0179,  0.8944,  0.8886]]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Head separation:\n",
      "Q1 (Head 1): \n",
      "tensor([[ 0.2434,  0.4607, -0.5537],\n",
      "        [-0.5975, -0.5909, -0.6584],\n",
      "        [ 0.4812, -0.1247,  0.3195]], grad_fn=<SliceBackward0>)\n",
      "Q2 (Head 2): \n",
      "tensor([[-0.5116, -0.0451,  0.1184],\n",
      "        [-0.2954, -0.6365, -0.7123],\n",
      "        [ 1.0179,  0.8944,  0.8886]], grad_fn=<SliceBackward0>)\n",
      "\n",
      "K1 (Head 1): \n",
      "tensor([[-0.3222,  0.3691,  0.3103],\n",
      "        [-0.5679,  0.7716,  0.3563],\n",
      "        [ 0.5660,  0.5104, -0.6236]], grad_fn=<SliceBackward0>)\n",
      "K2 (Head 2): \n",
      "tensor([[-0.5221, -0.0345,  0.4966],\n",
      "        [-0.4399,  1.3386,  0.2529],\n",
      "        [ 1.3696, -0.8633, -0.0945]], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Dimension verification:\n",
      "- Batch size: 1\n",
      "- Number of heads: 2\n",
      "- Number of tokens: 3\n",
      "- Head dimension: 3\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Transpose to group by heads instead of tokens\n",
    "# Transform from [B, T, H, head_dim] to [B, H, T, head_dim]\n",
    "\n",
    "Q_heads = Q_reshaped.transpose(1, 2)  # [1, 2, 3, 3]\n",
    "K_heads = K_reshaped.transpose(1, 2)  # [1, 2, 3, 3]\n",
    "V_heads = V_reshaped.transpose(1, 2)  # [1, 2, 3, 3]\n",
    "\n",
    "print(f\"After transpose (grouped by heads):\")\n",
    "print(f\"Q_heads shape: {Q_heads.shape}\")\n",
    "print(f\"K_heads shape: {K_heads.shape}\")\n",
    "print(f\"V_heads shape: {V_heads.shape}\")\n",
    "\n",
    "print(f\"\\nQ_heads structure:\")\n",
    "print(f\"Q_heads:\\n{Q_heads}\")\n",
    "\n",
    "print(f\"\\nHead separation:\")\n",
    "print(f\"Q1 (Head 1): \\n{Q_heads[0, 0, :, :]}\")  # All tokens for head 1\n",
    "print(f\"Q2 (Head 2): \\n{Q_heads[0, 1, :, :]}\")  # All tokens for head 2\n",
    "\n",
    "print(f\"\\nK1 (Head 1): \\n{K_heads[0, 0, :, :]}\")  # All tokens for head 1\n",
    "print(f\"K2 (Head 2): \\n{K_heads[0, 1, :, :]}\")  # All tokens for head 2\n",
    "\n",
    "# Verify we now have clear head groupings\n",
    "print(f\"\\nDimension verification:\")\n",
    "print(f\"- Batch size: {Q_heads.shape[0]}\")\n",
    "print(f\"- Number of heads: {Q_heads.shape[1]}\")\n",
    "print(f\"- Number of tokens: {Q_heads.shape[2]}\")\n",
    "print(f\"- Head dimension: {Q_heads.shape[3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4445b5",
   "metadata": {},
   "source": [
    "## Step 5: Compute Attention Scores for Each Head\n",
    "\n",
    "### The Parallel Computation\n",
    "**Objective**: Compute attention scores for each head independently\n",
    "\n",
    "**Mathematical operation**:\n",
    "```python\n",
    "# Head 1: Q1 @ K1.T\n",
    "attention_scores_1 = Q1 @ K1.transpose(-2, -1)\n",
    "\n",
    "# Head 2: Q2 @ K2.T  \n",
    "attention_scores_2 = Q2 @ K2.transpose(-2, -1)\n",
    "```\n",
    "\n",
    "### Batch Operation Magic\n",
    "**Instead of separate operations**, PyTorch handles all heads simultaneously:\n",
    "```python\n",
    "# All heads at once\n",
    "attention_scores = Q_heads @ K_heads.transpose(-2, -1)\n",
    "```\n",
    "\n",
    "### Dimensional Analysis\n",
    "**Input dimensions**:\n",
    "- Q_heads: `[B, H, T, head_dim]` = `[1, 2, 3, 3]`\n",
    "- K_heads.T: `[B, H, head_dim, T]` = `[1, 2, 3, 3]`\n",
    "\n",
    "**Matrix multiplication**:\n",
    "- `[B, H, T, head_dim] @ [B, H, head_dim, T]`\n",
    "- Result: `[B, H, T, T]` = `[1, 2, 3, 3]`\n",
    "\n",
    "### Key Insight\n",
    "**Attention scores dimensions**: Always `[num_tokens × num_tokens]` regardless of head_dim\n",
    "- Represents token-to-token relationships\n",
    "- Same for each head, but different values\n",
    "- Each head captures different perspective of relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de0bfd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores shape: torch.Size([1, 2, 3, 3])\n",
      "Expected: [batch_size, num_heads, num_tokens, num_tokens]\n",
      "Actual: [1, 2, 3, 3]\n",
      "\n",
      "Attention scores:\n",
      "tensor([[[[-0.0802,  0.0199,  0.7182],\n",
      "          [-0.2299, -0.3512, -0.2293],\n",
      "          [-0.1020, -0.2557,  0.0095]],\n",
      "\n",
      "         [[ 0.3275,  0.1947, -0.6730],\n",
      "          [-0.1775, -0.9021,  0.2122],\n",
      "          [-0.1211,  0.9741,  0.5378]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "Head 1 attention scores:\n",
      "tensor([[-0.0802,  0.0199,  0.7182],\n",
      "        [-0.2299, -0.3512, -0.2293],\n",
      "        [-0.1020, -0.2557,  0.0095]], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Head 2 attention scores:\n",
      "tensor([[ 0.3275,  0.1947, -0.6730],\n",
      "        [-0.1775, -0.9021,  0.2122],\n",
      "        [-0.1211,  0.9741,  0.5378]], grad_fn=<SliceBackward0>)\n",
      "\n",
      "✓ Attention scores have correct dimensions\n",
      "\n",
      "Manual Head 1 computation verification:\n",
      "PyTorch result:\n",
      "tensor([[-0.0802,  0.0199,  0.7182],\n",
      "        [-0.2299, -0.3512, -0.2293],\n",
      "        [-0.1020, -0.2557,  0.0095]], grad_fn=<SliceBackward0>)\n",
      "Manual result:\n",
      "tensor([[-0.0802,  0.0199,  0.7182],\n",
      "        [-0.2299, -0.3512, -0.2293],\n",
      "        [-0.1020, -0.2557,  0.0095]], grad_fn=<MmBackward0>)\n",
      "Equal: True\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Compute attention scores for each head\n",
    "# Q_heads @ K_heads.transpose(-2, -1)\n",
    "\n",
    "attention_scores = Q_heads @ K_heads.transpose(-2, -1)\n",
    "\n",
    "print(f\"Attention scores shape: {attention_scores.shape}\")\n",
    "print(f\"Expected: [batch_size, num_heads, num_tokens, num_tokens]\")\n",
    "print(f\"Actual: {list(attention_scores.shape)}\")\n",
    "\n",
    "print(f\"\\nAttention scores:\\n{attention_scores}\")\n",
    "\n",
    "print(f\"\\nHead 1 attention scores:\")\n",
    "print(f\"{attention_scores[0, 0, :, :]}\")\n",
    "\n",
    "print(f\"\\nHead 2 attention scores:\")\n",
    "print(f\"{attention_scores[0, 1, :, :]}\")\n",
    "\n",
    "# Verify dimensions\n",
    "assert attention_scores.shape == (batch_size, num_heads, num_tokens, num_tokens)\n",
    "print(f\"\\n✓ Attention scores have correct dimensions\")\n",
    "\n",
    "# Manual verification for Head 1\n",
    "Q1 = Q_heads[0, 0, :, :]  # [3, 3]\n",
    "K1 = K_heads[0, 0, :, :]  # [3, 3]\n",
    "manual_scores_1 = Q1 @ K1.T\n",
    "print(f\"\\nManual Head 1 computation verification:\")\n",
    "print(f\"PyTorch result:\\n{attention_scores[0, 0, :, :]}\")\n",
    "print(f\"Manual result:\\n{manual_scores_1}\")\n",
    "print(f\"Equal: {torch.allclose(attention_scores[0, 0, :, :], manual_scores_1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40582199",
   "metadata": {},
   "source": [
    "## Step 6: Apply Scaling, Masking, and Softmax\n",
    "\n",
    "### The Processing Pipeline\n",
    "1. **Scale** by √(head_dim) to prevent exploding values\n",
    "2. **Apply causal mask** to prevent looking into the future\n",
    "3. **Apply softmax** to normalize to probabilities\n",
    "4. **Optional dropout** for regularization\n",
    "\n",
    "### Scaling Rationale\n",
    "**Problem**: As head_dim increases, dot products become larger\n",
    "**Solution**: Divide by √(head_dim) to maintain variance ≈ 1\n",
    "\n",
    "```python\n",
    "scaled_scores = attention_scores / math.sqrt(head_dim)\n",
    "```\n",
    "\n",
    "### Causal Masking Implementation\n",
    "**Objective**: Set upper triangular elements to -∞\n",
    "**Result**: After softmax, these become 0 (can't attend to future)\n",
    "\n",
    "```\n",
    "Original:     After masking:\n",
    "[a  b  c]     [a  -∞  -∞]\n",
    "[d  e  f] →   [d   e  -∞]\n",
    "[g  h  i]     [g   h   i]\n",
    "```\n",
    "\n",
    "### Softmax Properties\n",
    "- **Row normalization**: Each row sums to 1\n",
    "- **-∞ handling**: e^(-∞) = 0\n",
    "- **Probability interpretation**: Attention weights as probabilities\n",
    "\n",
    "### Mathematical Sequence\n",
    "```\n",
    "scores → scale → mask → softmax → weights\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ada993ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled scores shape: torch.Size([1, 2, 3, 3])\n",
      "Scaling factor: 1/√3 = 0.5774\n",
      "\n",
      "Causal mask:\n",
      "tensor([[False,  True,  True],\n",
      "        [False, False,  True],\n",
      "        [False, False, False]])\n",
      "True = mask (set to -∞), False = keep\n",
      "\n",
      "Masked scores shape: torch.Size([1, 2, 3, 3])\n",
      "\n",
      "Head 1 - Before masking:\n",
      "tensor([[-0.0463,  0.0115,  0.4146],\n",
      "        [-0.1327, -0.2028, -0.1324],\n",
      "        [-0.0589, -0.1476,  0.0055]], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Head 1 - After masking:\n",
      "tensor([[-0.0463,    -inf,    -inf],\n",
      "        [-0.1327, -0.2028,    -inf],\n",
      "        [-0.0589, -0.1476,  0.0055]], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Head 2 - Before masking:\n",
      "tensor([[ 0.1891,  0.1124, -0.3885],\n",
      "        [-0.1025, -0.5208,  0.1225],\n",
      "        [-0.0699,  0.5624,  0.3105]], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Head 2 - After masking:\n",
      "tensor([[ 0.1891,    -inf,    -inf],\n",
      "        [-0.1025, -0.5208,    -inf],\n",
      "        [-0.0699,  0.5624,  0.3105]], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Attention weights shape: torch.Size([1, 2, 3, 3])\n",
      "\n",
      "Head 1 attention weights:\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5175, 0.4825, 0.0000],\n",
      "        [0.3354, 0.3069, 0.3577]], grad_fn=<SliceBackward0>)\n",
      "Row sums: tensor([1.0000, 1.0000, 1.0000], grad_fn=<SumBackward1>)\n",
      "\n",
      "Head 2 attention weights:\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.6031, 0.3969, 0.0000],\n",
      "        [0.2302, 0.4331, 0.3367]], grad_fn=<SliceBackward0>)\n",
      "Row sums: tensor([1.0000, 1.0000, 1.0000], grad_fn=<SumBackward1>)\n",
      "\n",
      "✓ All rows sum to 1 (proper probability distributions)\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Apply scaling, causal masking, and softmax\n",
    "\n",
    "# 6a. Scale by square root of head dimension\n",
    "scaled_scores = attention_scores / math.sqrt(head_dim)\n",
    "print(f\"Scaled scores shape: {scaled_scores.shape}\")\n",
    "print(f\"Scaling factor: 1/√{head_dim} = {1/math.sqrt(head_dim):.4f}\")\n",
    "\n",
    "# 6b. Create causal mask (upper triangular = True)\n",
    "causal_mask = torch.triu(torch.ones(num_tokens, num_tokens), diagonal=1).bool()\n",
    "print(f\"\\nCausal mask:\\n{causal_mask}\")\n",
    "print(f\"True = mask (set to -∞), False = keep\")\n",
    "\n",
    "# 6c. Apply causal mask\n",
    "masked_scores = scaled_scores.masked_fill(causal_mask, float('-inf'))\n",
    "print(f\"\\nMasked scores shape: {masked_scores.shape}\")\n",
    "\n",
    "print(f\"\\nHead 1 - Before masking:\")\n",
    "print(f\"{scaled_scores[0, 0, :, :]}\")\n",
    "print(f\"\\nHead 1 - After masking:\")\n",
    "print(f\"{masked_scores[0, 0, :, :]}\")\n",
    "\n",
    "print(f\"\\nHead 2 - Before masking:\")\n",
    "print(f\"{scaled_scores[0, 1, :, :]}\")\n",
    "print(f\"\\nHead 2 - After masking:\")\n",
    "print(f\"{masked_scores[0, 1, :, :]}\")\n",
    "\n",
    "# 6d. Apply softmax to get attention weights\n",
    "attention_weights = F.softmax(masked_scores, dim=-1)\n",
    "print(f\"\\nAttention weights shape: {attention_weights.shape}\")\n",
    "\n",
    "print(f\"\\nHead 1 attention weights:\")\n",
    "print(f\"{attention_weights[0, 0, :, :]}\")\n",
    "print(f\"Row sums: {attention_weights[0, 0, :, :].sum(dim=-1)}\")\n",
    "\n",
    "print(f\"\\nHead 2 attention weights:\")\n",
    "print(f\"{attention_weights[0, 1, :, :]}\")\n",
    "print(f\"Row sums: {attention_weights[0, 1, :, :].sum(dim=-1)}\")\n",
    "\n",
    "# Verify properties\n",
    "assert torch.allclose(attention_weights[0, 0, :, :].sum(dim=-1), torch.ones(num_tokens))\n",
    "assert torch.allclose(attention_weights[0, 1, :, :].sum(dim=-1), torch.ones(num_tokens))\n",
    "print(f\"\\n✓ All rows sum to 1 (proper probability distributions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb95da8",
   "metadata": {},
   "source": [
    "## Step 7: Compute Context Vectors for Each Head\n",
    "\n",
    "### The Final Transformation\n",
    "**Objective**: Convert attention weights into context vectors using value matrices\n",
    "\n",
    "**Mathematical operation**:\n",
    "```python\n",
    "# For each head: attention_weights @ values\n",
    "context_vectors = attention_weights @ V_heads\n",
    "```\n",
    "\n",
    "### Dimensional Analysis\n",
    "**Input dimensions**:\n",
    "- attention_weights: `[B, H, T, T]` = `[1, 2, 3, 3]`\n",
    "- V_heads: `[B, H, T, head_dim]` = `[1, 2, 3, 3]`\n",
    "\n",
    "**Matrix multiplication**:\n",
    "- `[B, H, T, T] @ [B, H, T, head_dim]`\n",
    "- Result: `[B, H, T, head_dim]` = `[1, 2, 3, 3]`\n",
    "\n",
    "### Interpretation\n",
    "**Context vector meaning**:\n",
    "- Each token gets a context vector per head\n",
    "- Context vector = weighted combination of all value vectors\n",
    "- Weights determined by attention scores\n",
    "- Different heads → different perspectives\n",
    "\n",
    "### Head-Specific Context\n",
    "- **Head 1 context**: V_heads weighted by attention_weights from Head 1\n",
    "- **Head 2 context**: V_heads weighted by attention_weights from Head 2\n",
    "- **Each head captures**: Different aspect of token relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "752853ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vectors shape: torch.Size([1, 2, 3, 3])\n",
      "Expected: [batch_size, num_heads, num_tokens, head_dim]\n",
      "\n",
      "Context vectors:\n",
      "tensor([[[[-0.8460,  0.2317,  0.0061],\n",
      "          [ 0.2524, -0.1025,  0.5735],\n",
      "          [ 0.0355,  0.4801, -0.2450]],\n",
      "\n",
      "         [[-0.1790,  0.0405,  0.0707],\n",
      "          [ 0.3812,  0.0439, -0.1098],\n",
      "          [ 0.3663,  0.3066,  0.0614]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "\n",
      "Head 1 context vectors:\n",
      "tensor([[-0.8460,  0.2317,  0.0061],\n",
      "        [ 0.2524, -0.1025,  0.5735],\n",
      "        [ 0.0355,  0.4801, -0.2450]], grad_fn=<SliceBackward0>)\n",
      "Shape: torch.Size([3, 3])\n",
      "\n",
      "Head 2 context vectors:\n",
      "tensor([[-0.1790,  0.0405,  0.0707],\n",
      "        [ 0.3812,  0.0439, -0.1098],\n",
      "        [ 0.3663,  0.3066,  0.0614]], grad_fn=<SliceBackward0>)\n",
      "Shape: torch.Size([3, 3])\n",
      "\n",
      "Interpretation:\n",
      "- Token 1, Head 1 context: tensor([-0.8460,  0.2317,  0.0061], grad_fn=<SliceBackward0>)\n",
      "- Token 1, Head 2 context: tensor([-0.1790,  0.0405,  0.0707], grad_fn=<SliceBackward0>)\n",
      "- Token 2, Head 1 context: tensor([ 0.2524, -0.1025,  0.5735], grad_fn=<SliceBackward0>)\n",
      "- Token 2, Head 2 context: tensor([ 0.3812,  0.0439, -0.1098], grad_fn=<SliceBackward0>)\n",
      "\n",
      "✓ Context vectors have correct dimensions\n",
      "\n",
      "Manual verification for Token 1, Head 1:\n",
      "Manual computation: tensor([-0.8460,  0.2317,  0.0061], grad_fn=<SqueezeBackward4>)\n",
      "PyTorch result: tensor([-0.8460,  0.2317,  0.0061], grad_fn=<SliceBackward0>)\n",
      "Equal: True\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Compute context vectors for each head\n",
    "# attention_weights @ V_heads\n",
    "\n",
    "context_vectors = attention_weights @ V_heads\n",
    "print(f\"Context vectors shape: {context_vectors.shape}\")\n",
    "print(f\"Expected: [batch_size, num_heads, num_tokens, head_dim]\")\n",
    "\n",
    "print(f\"\\nContext vectors:\\n{context_vectors}\")\n",
    "\n",
    "print(f\"\\nHead 1 context vectors:\")\n",
    "print(f\"{context_vectors[0, 0, :, :]}\")\n",
    "print(f\"Shape: {context_vectors[0, 0, :, :].shape}\")\n",
    "\n",
    "print(f\"\\nHead 2 context vectors:\")\n",
    "print(f\"{context_vectors[0, 1, :, :]}\")\n",
    "print(f\"Shape: {context_vectors[0, 1, :, :].shape}\")\n",
    "\n",
    "# Interpret the results\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"- Token 1, Head 1 context: {context_vectors[0, 0, 0, :]}\")\n",
    "print(f\"- Token 1, Head 2 context: {context_vectors[0, 1, 0, :]}\")\n",
    "print(f\"- Token 2, Head 1 context: {context_vectors[0, 0, 1, :]}\")\n",
    "print(f\"- Token 2, Head 2 context: {context_vectors[0, 1, 1, :]}\")\n",
    "\n",
    "# Verify dimensions\n",
    "assert context_vectors.shape == (batch_size, num_heads, num_tokens, head_dim)\n",
    "print(f\"\\n✓ Context vectors have correct dimensions\")\n",
    "\n",
    "# Manual verification for one computation\n",
    "print(f\"\\nManual verification for Token 1, Head 1:\")\n",
    "manual_context = attention_weights[0, 0, 0, :] @ V_heads[0, 0, :, :]\n",
    "print(f\"Manual computation: {manual_context}\")\n",
    "print(f\"PyTorch result: {context_vectors[0, 0, 0, :]}\")\n",
    "print(f\"Equal: {torch.allclose(manual_context, context_vectors[0, 0, 0, :])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5917a4c",
   "metadata": {},
   "source": [
    "## Step 8: Merge Heads (Concatenation)\n",
    "\n",
    "### The Merging Process\n",
    "**Objective**: Combine all head outputs into single context matrix\n",
    "\n",
    "**Current structure**: `[B, H, T, head_dim]` - grouped by heads\n",
    "**Target structure**: `[B, T, d_out]` - grouped by tokens\n",
    "\n",
    "### Two-Step Process\n",
    "\n",
    "#### Step 8a: Transpose Back to Token Grouping\n",
    "```python\n",
    "# [B, H, T, head_dim] → [B, T, H, head_dim]\n",
    "context_vectors.transpose(1, 2)\n",
    "```\n",
    "\n",
    "#### Step 8b: Flatten Head Dimensions\n",
    "```python\n",
    "# [B, T, H, head_dim] → [B, T, H * head_dim]\n",
    "# [B, T, 2, 3] → [B, T, 6]\n",
    ".contiguous().view(B, T, d_out)\n",
    "```\n",
    "\n",
    "### Visual Understanding\n",
    "\n",
    "**Before merging** (grouped by heads):\n",
    "```\n",
    "Head 1:\n",
    "  Token 1: [c₁₁, c₁₂, c₁₃]\n",
    "  Token 2: [c₂₁, c₂₂, c₂₃]\n",
    "  Token 3: [c₃₁, c₃₂, c₃₃]\n",
    "\n",
    "Head 2:\n",
    "  Token 1: [c₁₄, c₁₅, c₁₆]\n",
    "  Token 2: [c₂₄, c₂₅, c₂₆]\n",
    "  Token 3: [c₃₄, c₃₅, c₃₆]\n",
    "```\n",
    "\n",
    "**After merging** (grouped by tokens):\n",
    "```\n",
    "Token 1: [c₁₁, c₁₂, c₁₃, c₁₄, c₁₅, c₁₆]  # Head 1 + Head 2\n",
    "Token 2: [c₂₁, c₂₂, c₂₃, c₂₄, c₂₅, c₂₆]  # Head 1 + Head 2\n",
    "Token 3: [c₃₁, c₃₂, c₃₃, c₃₄, c₃₅, c₃₆]  # Head 1 + Head 2\n",
    "```\n",
    "\n",
    "### Final Result Properties\n",
    "- **Same dimensions** as single-head attention output\n",
    "- **Richer content**: Multiple perspectives embedded\n",
    "- **Seamless integration**: Can replace single-head attention directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3f84019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After transpose back to token grouping:\n",
      "Shape: torch.Size([1, 3, 2, 3])\n",
      "Expected: [batch_size, num_tokens, num_heads, head_dim]\n",
      "\n",
      "Context transposed:\n",
      "tensor([[[[-0.8460,  0.2317,  0.0061],\n",
      "          [-0.1790,  0.0405,  0.0707]],\n",
      "\n",
      "         [[ 0.2524, -0.1025,  0.5735],\n",
      "          [ 0.3812,  0.0439, -0.1098]],\n",
      "\n",
      "         [[ 0.0355,  0.4801, -0.2450],\n",
      "          [ 0.3663,  0.3066,  0.0614]]]], grad_fn=<TransposeBackward0>)\n",
      "\n",
      "Token-wise view:\n",
      "Token 1: Head 1 = tensor([-0.8460,  0.2317,  0.0061], grad_fn=<SliceBackward0>), Head 2 = tensor([-0.1790,  0.0405,  0.0707], grad_fn=<SliceBackward0>)\n",
      "Token 2: Head 1 = tensor([ 0.2524, -0.1025,  0.5735], grad_fn=<SliceBackward0>), Head 2 = tensor([ 0.3812,  0.0439, -0.1098], grad_fn=<SliceBackward0>)\n",
      "Token 3: Head 1 = tensor([ 0.0355,  0.4801, -0.2450], grad_fn=<SliceBackward0>), Head 2 = tensor([0.3663, 0.3066, 0.0614], grad_fn=<SliceBackward0>)\n",
      "\n",
      "Final context matrix:\n",
      "Shape: torch.Size([1, 3, 6])\n",
      "Expected: [batch_size, num_tokens, d_out] = [1, 3, 6]\n",
      "\n",
      "Final context:\n",
      "tensor([[[-0.8460,  0.2317,  0.0061, -0.1790,  0.0405,  0.0707],\n",
      "         [ 0.2524, -0.1025,  0.5735,  0.3812,  0.0439, -0.1098],\n",
      "         [ 0.0355,  0.4801, -0.2450,  0.3663,  0.3066,  0.0614]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "Final context breakdown:\n",
      "Token 1 final context: tensor([-0.8460,  0.2317,  0.0061, -0.1790,  0.0405,  0.0707],\n",
      "       grad_fn=<SliceBackward0>) (Head1: tensor([-0.8460,  0.2317,  0.0061], grad_fn=<SliceBackward0>), Head2: tensor([-0.1790,  0.0405,  0.0707], grad_fn=<SliceBackward0>))\n",
      "Token 2 final context: tensor([ 0.2524, -0.1025,  0.5735,  0.3812,  0.0439, -0.1098],\n",
      "       grad_fn=<SliceBackward0>) (Head1: tensor([ 0.2524, -0.1025,  0.5735], grad_fn=<SliceBackward0>), Head2: tensor([ 0.3812,  0.0439, -0.1098], grad_fn=<SliceBackward0>))\n",
      "Token 3 final context: tensor([ 0.0355,  0.4801, -0.2450,  0.3663,  0.3066,  0.0614],\n",
      "       grad_fn=<SliceBackward0>) (Head1: tensor([ 0.0355,  0.4801, -0.2450], grad_fn=<SliceBackward0>), Head2: tensor([0.3663, 0.3066, 0.0614], grad_fn=<SliceBackward0>))\n",
      "\n",
      "✓ Final context has correct dimensions: torch.Size([1, 3, 6])\n",
      "\n",
      "Dimension comparison:\n",
      "Single-head output would be: [1, 3, 6]\n",
      "Multi-head output is:        [1, 3, 6]\n",
      "Same dimensions: True\n",
      "But multi-head contains multiple perspectives!\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Merge heads by concatenating head outputs\n",
    "\n",
    "# 8a. Transpose back to group by tokens\n",
    "# [B, H, T, head_dim] → [B, T, H, head_dim]\n",
    "context_transposed = context_vectors.transpose(1, 2)\n",
    "print(f\"After transpose back to token grouping:\")\n",
    "print(f\"Shape: {context_transposed.shape}\")\n",
    "print(f\"Expected: [batch_size, num_tokens, num_heads, head_dim]\")\n",
    "\n",
    "print(f\"\\nContext transposed:\\n{context_transposed}\")\n",
    "\n",
    "print(f\"\\nToken-wise view:\")\n",
    "print(f\"Token 1: Head 1 = {context_transposed[0, 0, 0, :]}, Head 2 = {context_transposed[0, 0, 1, :]}\")\n",
    "print(f\"Token 2: Head 1 = {context_transposed[0, 1, 0, :]}, Head 2 = {context_transposed[0, 1, 1, :]}\")\n",
    "print(f\"Token 3: Head 1 = {context_transposed[0, 2, 0, :]}, Head 2 = {context_transposed[0, 2, 1, :]}\")\n",
    "\n",
    "# 8b. Flatten the head dimensions to concatenate\n",
    "# [B, T, H, head_dim] → [B, T, H * head_dim] = [B, T, d_out]\n",
    "final_context = context_transposed.contiguous().view(batch_size, num_tokens, d_out)\n",
    "\n",
    "print(f\"\\nFinal context matrix:\")\n",
    "print(f\"Shape: {final_context.shape}\")\n",
    "print(f\"Expected: [batch_size, num_tokens, d_out] = [1, 3, 6]\")\n",
    "\n",
    "print(f\"\\nFinal context:\\n{final_context}\")\n",
    "\n",
    "print(f\"\\nFinal context breakdown:\")\n",
    "print(f\"Token 1 final context: {final_context[0, 0, :]} (Head1: {final_context[0, 0, :3]}, Head2: {final_context[0, 0, 3:]})\")\n",
    "print(f\"Token 2 final context: {final_context[0, 1, :]} (Head1: {final_context[0, 1, :3]}, Head2: {final_context[0, 1, 3:]})\")\n",
    "print(f\"Token 3 final context: {final_context[0, 2, :]} (Head1: {final_context[0, 2, :3]}, Head2: {final_context[0, 2, 3:]})\")\n",
    "\n",
    "# Verify final dimensions\n",
    "assert final_context.shape == (batch_size, num_tokens, d_out)\n",
    "print(f\"\\n✓ Final context has correct dimensions: {final_context.shape}\")\n",
    "\n",
    "# Compare to single-head output dimensions\n",
    "print(f\"\\nDimension comparison:\")\n",
    "print(f\"Single-head output would be: [1, 3, 6]\")\n",
    "print(f\"Multi-head output is:        {list(final_context.shape)}\")\n",
    "print(f\"Same dimensions: {final_context.shape == (1, 3, 6)}\")\n",
    "print(f\"But multi-head contains multiple perspectives!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34345464",
   "metadata": {},
   "source": [
    "## Complete Multi-Head Attention Implementation\n",
    "\n",
    "### Full Class Implementation\n",
    "Now let's implement the complete multi-head attention class that encapsulates all the steps we've seen:\n",
    "\n",
    "```python\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        \n",
    "        # Linear layers for Q, K, V transformations\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=False)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Register causal mask as buffer\n",
    "        self.register_buffer(\n",
    "            'mask', \n",
    "            torch.triu(torch.ones(1000, 1000), diagonal=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Implementation of all 8 steps\n",
    "        ...\n",
    "```\n",
    "\n",
    "### Key Implementation Details\n",
    "1. **Efficient mask handling**: Pre-registered buffer\n",
    "2. **Flexible sequence length**: Mask larger than needed\n",
    "3. **Dropout integration**: Optional regularization\n",
    "4. **Batch processing**: Handles multiple batches efficiently\n",
    "5. **Memory efficiency**: In-place operations where possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f363627b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "masked_fill_ only supports boolean masks, but got mask with dtype float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 69\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Test with our example input\u001b[39;00m\n\u001b[0;32m     68\u001b[0m test_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m6\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiHeadAttention test:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_input\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\sayus\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sayus\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[9], line 49\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Step 6: Apply causal mask\u001b[39;00m\n\u001b[0;32m     48\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask[:num_tokens, :num_tokens]\n\u001b[1;32m---> 49\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mattention_scores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-inf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Step 7: Apply softmax and dropout\u001b[39;00m\n\u001b[0;32m     52\u001b[0m attention_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(attention_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: masked_fill_ only supports boolean masks, but got mask with dtype float"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, num_heads, context_length=1000, dropout=0.0):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        \n",
    "        # Linear transformations for Q, K, V\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=False)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=False)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Register causal mask buffer\n",
    "        self.register_buffer('mask', \n",
    "                            torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Input shape: [batch_size, num_tokens, d_in]\n",
    "        batch_size, num_tokens, d_in = x.shape\n",
    "        \n",
    "        # Step 1: Generate Q, K, V matrices\n",
    "        Q = self.W_query(x)  # [batch_size, num_tokens, d_out]\n",
    "        K = self.W_key(x)    # [batch_size, num_tokens, d_out]\n",
    "        V = self.W_value(x)  # [batch_size, num_tokens, d_out]\n",
    "        \n",
    "        # Step 2: Reshape for multiple heads\n",
    "        Q = Q.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n",
    "        K = K.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n",
    "        V = V.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # Step 3: Transpose to group by heads\n",
    "        Q = Q.transpose(1, 2)  # [batch_size, num_heads, num_tokens, head_dim]\n",
    "        K = K.transpose(1, 2)  # [batch_size, num_heads, num_tokens, head_dim]\n",
    "        V = V.transpose(1, 2)  # [batch_size, num_heads, num_tokens, head_dim]\n",
    "        \n",
    "        # Step 4: Compute attention scores\n",
    "        attention_scores = Q @ K.transpose(-2, -1)  # [batch_size, num_heads, num_tokens, num_tokens]\n",
    "        \n",
    "        # Step 5: Scale scores\n",
    "        attention_scores = attention_scores / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Step 6: Apply causal mask\n",
    "        mask = self.mask[:num_tokens, :num_tokens]\n",
    "        attention_scores = attention_scores.masked_fill(mask, float('-inf'))\n",
    "        \n",
    "        # Step 7: Apply softmax and dropout\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        \n",
    "        # Step 8: Compute context vectors\n",
    "        context_vectors = attention_weights @ V  # [batch_size, num_heads, num_tokens, head_dim]\n",
    "        \n",
    "        # Step 9: Transpose back and merge heads\n",
    "        context_vectors = context_vectors.transpose(1, 2)  # [batch_size, num_tokens, num_heads, head_dim]\n",
    "        context_vectors = context_vectors.contiguous().view(batch_size, num_tokens, self.d_out)\n",
    "        \n",
    "        return context_vectors\n",
    "\n",
    "# Test the implementation\n",
    "mha = MultiHeadAttention(d_in=6, d_out=6, num_heads=2, dropout=0.0)\n",
    "\n",
    "# Test with our example input\n",
    "test_input = torch.randn(1, 3, 6)\n",
    "output = mha(test_input)\n",
    "\n",
    "print(f\"MultiHeadAttention test:\")\n",
    "print(f\"Input shape: {test_input.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output:\\n{output}\")\n",
    "\n",
    "# Test with multiple batches\n",
    "batch_input = torch.randn(2, 3, 6)\n",
    "batch_output = mha(batch_input)\n",
    "print(f\"\\nBatch test:\")\n",
    "print(f\"Batch input shape: {batch_input.shape}\")\n",
    "print(f\"Batch output shape: {batch_output.shape}\")\n",
    "\n",
    "# Verify our manual computation matches the class\n",
    "manual_output = final_context\n",
    "class_output = mha(X)\n",
    "print(f\"\\nVerification (manual vs class):\")\n",
    "print(f\"Manual output shape: {manual_output.shape}\")\n",
    "print(f\"Class output shape: {class_output.shape}\")\n",
    "print(f\"Shapes match: {manual_output.shape == class_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e285b56",
   "metadata": {},
   "source": [
    "## Performance Analysis and Key Insights\n",
    "\n",
    "### Computational Complexity Analysis\n",
    "\n",
    "#### Parameter Count Comparison\n",
    "**Single-head attention**:\n",
    "```\n",
    "WQ: d_in × d_out = 6 × 6 = 36 parameters\n",
    "WK: d_in × d_out = 6 × 6 = 36 parameters  \n",
    "WV: d_in × d_out = 6 × 6 = 36 parameters\n",
    "Total: 108 parameters\n",
    "```\n",
    "\n",
    "**Multi-head attention (2 heads)**:\n",
    "```\n",
    "Same weight matrices, just reshaped!\n",
    "Total: 108 parameters (no increase)\n",
    "```\n",
    "\n",
    "#### Memory Complexity\n",
    "**Attention matrices per head**: O(seq_len²)\n",
    "- Single head: 1 × [3 × 3] = 9 elements\n",
    "- Multi-head: 2 × [3 × 3] = 18 elements\n",
    "- **Memory scales linearly with number of heads**\n",
    "\n",
    "#### Time Complexity\n",
    "**Per head**: O(seq_len² × head_dim)\n",
    "**Total**: O(seq_len² × d_out) - same as single head!\n",
    "\n",
    "### Why Multi-Head Attention Works\n",
    "\n",
    "#### 1. **No Parameter Overhead**\n",
    "- Same parameter count as single-head\n",
    "- Just reorganized differently\n",
    "- Pure architectural innovation\n",
    "\n",
    "#### 2. **Parallel Perspective Learning**\n",
    "- Each head specializes in different patterns\n",
    "- Head 1: Syntactic relationships\n",
    "- Head 2: Semantic relationships  \n",
    "- Head N: Positional patterns\n",
    "\n",
    "#### 3. **Empirical Superiority**\n",
    "- Consistently outperforms single-head\n",
    "- Captures richer representations\n",
    "- Better generalization across tasks\n",
    "\n",
    "#### 4. **Scalability**\n",
    "- Easily scales to many heads (8, 12, 16+)\n",
    "- GPU-friendly parallel computation\n",
    "- Modular design for different architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb2efffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Count Analysis:\n",
      "==================================================\n",
      "Base parameters (WQ + WK + WV): 786,432\n",
      "Heads:  1, Head dim: 512, Parameters: 786,432\n",
      "Heads:  8, Head dim:  64, Parameters: 786,432\n",
      "Heads: 12, Head dim:  42, Parameters: 786,432\n",
      "Heads: 16, Head dim:  32, Parameters: 786,432\n",
      "\n",
      "Memory Analysis (for seq_len=1024):\n",
      "==================================================\n",
      "Heads:  1, Attention matrices: 1,048,576 elements, Memory: 4.0 MB\n",
      "Heads:  8, Attention matrices: 8,388,608 elements, Memory: 32.0 MB\n",
      "Heads: 12, Attention matrices: 12,582,912 elements, Memory: 48.0 MB\n",
      "Heads: 16, Attention matrices: 16,777,216 elements, Memory: 64.0 MB\n",
      "\n",
      "Computational Complexity Analysis:\n",
      "==================================================\n",
      "Heads:  1, Ops per head: 536,870,912, Total ops: 536,870,912\n",
      "Heads:  8, Ops per head: 67,108,864, Total ops: 536,870,912\n",
      "Heads: 12, Ops per head: 44,040,192, Total ops: 528,482,304\n",
      "Heads: 16, Ops per head: 33,554,432, Total ops: 536,870,912\n",
      "\n",
      "\n",
      "Head Specialization Demonstration:\n",
      "==================================================\n",
      "Multi-head attention enables models to capture:\n",
      "• Head 1: Syntactic relationships (subject-verb-object)\n",
      "• Head 2: Semantic relationships (word meanings)\n",
      "• Head 3: Positional patterns (sequence order)\n",
      "• Head 4: Long-range dependencies\n",
      "• Head 5: Entity relationships\n",
      "• Head 6: Discourse structure\n",
      "• Head 7: Attention to rare words\n",
      "• Head 8: Contextual disambiguation\n",
      "\n",
      "Each sentence can be analyzed from 8 different perspectives!\n",
      "This is why modern LLMs use 12-96 attention heads per layer.\n"
     ]
    }
   ],
   "source": [
    "# Performance analysis and comparisons\n",
    "\n",
    "def analyze_multihead_performance():\n",
    "    \"\"\"Analyze the performance characteristics of multi-head attention\"\"\"\n",
    "    \n",
    "    # Parameter count analysis\n",
    "    d_in, d_out = 512, 512  # Typical transformer dimensions\n",
    "    num_heads_options = [1, 8, 12, 16]\n",
    "    \n",
    "    print(\"Parameter Count Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    base_params = 3 * d_in * d_out  # WQ + WK + WV\n",
    "    print(f\"Base parameters (WQ + WK + WV): {base_params:,}\")\n",
    "    \n",
    "    for num_heads in num_heads_options:\n",
    "        head_dim = d_out // num_heads\n",
    "        # Parameters remain the same regardless of heads!\n",
    "        print(f\"Heads: {num_heads:2d}, Head dim: {head_dim:3d}, Parameters: {base_params:,}\")\n",
    "    \n",
    "    print(\"\\nMemory Analysis (for seq_len=1024):\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    seq_len = 1024\n",
    "    for num_heads in num_heads_options:\n",
    "        attention_matrices = num_heads * seq_len * seq_len\n",
    "        memory_mb = attention_matrices * 4 / (1024 * 1024)  # 4 bytes per float32\n",
    "        print(f\"Heads: {num_heads:2d}, Attention matrices: {attention_matrices:,} elements, Memory: {memory_mb:.1f} MB\")\n",
    "    \n",
    "    print(\"\\nComputational Complexity Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for num_heads in num_heads_options:\n",
    "        head_dim = d_out // num_heads\n",
    "        ops_per_head = seq_len * seq_len * head_dim\n",
    "        total_ops = num_heads * ops_per_head\n",
    "        # Note: total_ops = seq_len² × d_out regardless of num_heads!\n",
    "        print(f\"Heads: {num_heads:2d}, Ops per head: {ops_per_head:,}, Total ops: {total_ops:,}\")\n",
    "\n",
    "analyze_multihead_performance()\n",
    "\n",
    "# Demonstrate head specialization potential\n",
    "def demonstrate_head_specialization():\n",
    "    \"\"\"Show how different heads can capture different patterns\"\"\"\n",
    "    \n",
    "    print(\"\\n\\nHead Specialization Demonstration:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create a model with many heads to show specialization potential\n",
    "    specialized_mha = MultiHeadAttention(d_in=128, d_out=128, num_heads=8)\n",
    "    \n",
    "    # Example sentences that could benefit from different perspectives\n",
    "    sentences = [\n",
    "        \"The artist painted the portrait\",\n",
    "        \"The government should regulate speech\", \n",
    "        \"Time flies like an arrow\",\n",
    "        \"Bank accounts and river banks\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Multi-head attention enables models to capture:\")\n",
    "    print(\"• Head 1: Syntactic relationships (subject-verb-object)\")\n",
    "    print(\"• Head 2: Semantic relationships (word meanings)\")\n",
    "    print(\"• Head 3: Positional patterns (sequence order)\")\n",
    "    print(\"• Head 4: Long-range dependencies\")\n",
    "    print(\"• Head 5: Entity relationships\")\n",
    "    print(\"• Head 6: Discourse structure\")\n",
    "    print(\"• Head 7: Attention to rare words\")\n",
    "    print(\"• Head 8: Contextual disambiguation\")\n",
    "    \n",
    "    print(f\"\\nEach sentence can be analyzed from {specialized_mha.num_heads} different perspectives!\")\n",
    "    print(\"This is why modern LLMs use 12-96 attention heads per layer.\")\n",
    "\n",
    "demonstrate_head_specialization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cf4e83",
   "metadata": {},
   "source": [
    "## Connection to Modern Language Models\n",
    "\n",
    "### Multi-Head Attention in Practice\n",
    "\n",
    "#### GPT Models\n",
    "- **GPT-1**: 12 heads per layer, 12 layers\n",
    "- **GPT-2**: 12-25 heads per layer, 24-48 layers  \n",
    "- **GPT-3**: 96 heads per layer, 96 layers\n",
    "- **GPT-4**: Estimated 128+ heads per layer\n",
    "\n",
    "#### BERT Models\n",
    "- **BERT-Base**: 12 heads per layer, 12 layers\n",
    "- **BERT-Large**: 16 heads per layer, 24 layers\n",
    "\n",
    "#### Other Architectures\n",
    "- **T5**: 12-32 heads depending on size\n",
    "- **PaLM**: 64 heads per layer\n",
    "- **DeepSeek**: Advanced multi-head latent attention\n",
    "\n",
    "### Research Insights on Head Specialization\n",
    "\n",
    "#### Empirical Findings\n",
    "1. **Syntactic heads**: Learn grammatical relationships\n",
    "2. **Semantic heads**: Focus on word meanings and concepts\n",
    "3. **Positional heads**: Track sequence order and position\n",
    "4. **Rare word heads**: Pay attention to uncommon tokens\n",
    "5. **Long-range heads**: Capture distant dependencies\n",
    "\n",
    "#### Head Pruning Studies\n",
    "- Many heads can be removed without significant performance loss\n",
    "- Some heads are more critical than others\n",
    "- Redundancy provides robustness\n",
    "\n",
    "### The Path to Advanced Architectures\n",
    "\n",
    "#### Current Position in Learning Journey\n",
    "1. ✅ **Self-attention**: Foundation mechanism\n",
    "2. ✅ **Causal attention**: Prevents future information leakage  \n",
    "3. ✅ **Multi-head attention**: Multiple perspectives\n",
    "4. 🔄 **Next: Key-Value caching** - Efficiency optimization\n",
    "5. 🔄 **Final: Multi-head latent attention** - DeepSeek's innovation\n",
    "\n",
    "#### Why This Foundation Matters\n",
    "- **KV caching**: Optimizes the K,V computations we just learned\n",
    "- **Flash attention**: Memory-efficient attention computation\n",
    "- **Multi-head latent attention**: Compresses multi-head structure\n",
    "- **Understanding prerequisites**: Advanced concepts build on these basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe74fd37",
   "metadata": {},
   "source": [
    "## Key Takeaways and Summary\n",
    "\n",
    "### Core Concepts Mastered\n",
    "\n",
    "#### 1. **Mathematical Implementation**\n",
    "- **8-step process**: From input to final context vectors\n",
    "- **Dimensional transformations**: Tracking shapes through all operations\n",
    "- **Matrix operations**: Understanding every multiplication and transpose\n",
    "\n",
    "#### 2. **Multi-Head Mechanism**\n",
    "- **Parameter efficiency**: No increase in parameter count\n",
    "- **Perspective diversity**: Each head captures different relationships\n",
    "- **Parallel processing**: All heads computed simultaneously\n",
    "\n",
    "#### 3. **Implementation Details**\n",
    "- **Reshaping operations**: view() for dimension changes\n",
    "- **Transpose operations**: Grouping by heads vs tokens\n",
    "- **Masking and normalization**: Causal attention integration\n",
    "\n",
    "### Critical Insights\n",
    "\n",
    "#### **The Magic of Multi-Head Attention**\n",
    "```\n",
    "Same parameters + Different organization = Multiple perspectives\n",
    "```\n",
    "\n",
    "#### **Dimensional Consistency**\n",
    "```\n",
    "Input:  [B, T, d_in]  → Processing → Output: [B, T, d_out]\n",
    "Single-head and multi-head have identical input/output shapes!\n",
    "```\n",
    "\n",
    "#### **Computational Efficiency**\n",
    "```\n",
    "Time complexity: O(seq_len² × d_out) - independent of number of heads\n",
    "Memory complexity: O(num_heads × seq_len²) - linear scaling\n",
    "```\n",
    "\n",
    "### Why This Matters for DeepSeek\n",
    "\n",
    "#### **Foundation for Advanced Concepts**\n",
    "- Multi-head attention is the baseline that DeepSeek improves upon\n",
    "- Key-Value caching optimizes the computation we just learned\n",
    "- Multi-head latent attention compresses the multi-head structure\n",
    "\n",
    "#### **Understanding Prerequisites**\n",
    "- Can't understand advanced optimizations without mastering the basics\n",
    "- Every modern LLM builds on these fundamental mechanisms\n",
    "- Critical for implementing and modifying transformer architectures\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "#### **Immediate Applications**\n",
    "- Implement transformer blocks using this multi-head attention\n",
    "- Experiment with different numbers of heads\n",
    "- Analyze attention patterns in pre-trained models\n",
    "\n",
    "#### **Advanced Topics Coming**\n",
    "- **Key-Value caching**: Memory and speed optimizations\n",
    "- **Flash attention**: Memory-efficient computation\n",
    "- **Multi-head latent attention**: DeepSeek's key innovation\n",
    "\n",
    "This multi-head attention implementation is the workhorse of modern AI - understanding it deeply is essential for anyone working with large language models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
